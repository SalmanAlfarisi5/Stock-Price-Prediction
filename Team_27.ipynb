{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "u8DYmzAqrDR8",
    "outputId": "096ca1a1-636d-4bc1-bb31-1221b814becb"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "import pmdarima as pm\n",
    "from pmdarima import auto_arima\n",
    "from arch import arch_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression, RFE, RFECV\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, ParameterSampler, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, GRU, Flatten, BatchNormalization, Dropout, Input, SimpleRNN, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback\n",
    "import random\n",
    "import pickle\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDMUDgZwusP3"
   },
   "source": [
    "# **Preprocessing Data + Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYsBrpbj4pIa"
   },
   "source": [
    "**Read and Combine dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ChI9ncpXsxvl"
   },
   "outputs": [],
   "source": [
    "# Reading the CSV files\n",
    "company_info_df = pd.read_csv('company_info.csv')\n",
    "company_stock_details_df = pd.read_csv('company_stock_details.csv')\n",
    "\n",
    "# Convert the Date column to datetime format\n",
    "company_stock_details_df['Date'] = pd.to_datetime(company_stock_details_df['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Combining the dataset\n",
    "data_combined = pd.merge(company_info_df, company_stock_details_df, on='Symbol', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9sSD8rn4_xI"
   },
   "source": [
    "**Replace NaN with zero for News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abuidVQIszxD",
    "outputId": "1e45bd5d-5b55-4648-8dfb-d6d8460b171f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol                             0\n",
      "GICS Sector                        0\n",
      "Headquarters Location              0\n",
      "Founded                            0\n",
      "Date                               2\n",
      "Close                              2\n",
      "Volume                             2\n",
      "News - Positive Sentiment        522\n",
      "News - Negative Sentiment        522\n",
      "News - New Products              522\n",
      "News - Layoffs                   522\n",
      "News - Analyst Comments          522\n",
      "News - Stocks                    522\n",
      "News - Dividends                 522\n",
      "News - Corporate Earnings        522\n",
      "News - Mergers & Acquisitions    522\n",
      "News - Store Openings            522\n",
      "News - Product Recalls           522\n",
      "News - Adverse Events            522\n",
      "News - Personnel Changes         522\n",
      "News - Stock Rumors              522\n",
      "dtype: int64\n",
      "      Symbol       GICS Sector Headquarters Location Founded Date  Close  \\\n",
      "29988  BRK.B        Financials       Omaha, Nebraska    1839  NaT    NaN   \n",
      "37045   BF.B  Consumer Staples  Louisville, Kentucky    1870  NaT    NaN   \n",
      "\n",
      "       Volume  News - Positive Sentiment  News - Negative Sentiment  \\\n",
      "29988     NaN                        NaN                        NaN   \n",
      "37045     NaN                        NaN                        NaN   \n",
      "\n",
      "       News - New Products  ...  News - Analyst Comments  News - Stocks  \\\n",
      "29988                  NaN  ...                      NaN            NaN   \n",
      "37045                  NaN  ...                      NaN            NaN   \n",
      "\n",
      "       News - Dividends  News - Corporate Earnings  \\\n",
      "29988               NaN                        NaN   \n",
      "37045               NaN                        NaN   \n",
      "\n",
      "       News - Mergers & Acquisitions  News - Store Openings  \\\n",
      "29988                            NaN                    NaN   \n",
      "37045                            NaN                    NaN   \n",
      "\n",
      "       News - Product Recalls  News - Adverse Events  \\\n",
      "29988                     NaN                    NaN   \n",
      "37045                     NaN                    NaN   \n",
      "\n",
      "       News - Personnel Changes  News - Stock Rumors  \n",
      "29988                       NaN                  NaN  \n",
      "37045                       NaN                  NaN  \n",
      "\n",
      "[2 rows x 21 columns]\n",
      "Symbol                           0\n",
      "GICS Sector                      0\n",
      "Headquarters Location            0\n",
      "Founded                          0\n",
      "Date                             0\n",
      "Close                            0\n",
      "Volume                           0\n",
      "News - Positive Sentiment        0\n",
      "News - Negative Sentiment        0\n",
      "News - New Products              0\n",
      "News - Layoffs                   0\n",
      "News - Analyst Comments          0\n",
      "News - Stocks                    0\n",
      "News - Dividends                 0\n",
      "News - Corporate Earnings        0\n",
      "News - Mergers & Acquisitions    0\n",
      "News - Store Openings            0\n",
      "News - Product Recalls           0\n",
      "News - Adverse Events            0\n",
      "News - Personnel Changes         0\n",
      "News - Stock Rumors              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing value\n",
    "print(data_combined.isnull().sum())\n",
    "\n",
    "# Display rows with missing values in \"Date\", \"Close\", or \"Volume\"\n",
    "missing_data_rows = data_combined[data_combined[['Date', 'Close', 'Volume']].isnull().any(axis=1)]\n",
    "print(missing_data_rows)\n",
    "\n",
    "# Remove rows with missing values in \"Date\", \"Close\", or \"Volume\"\n",
    "data_combined = data_combined.dropna(subset=['Date', 'Close', 'Volume'])\n",
    "\n",
    "# Change the missing news volume to 0\n",
    "data_combined.fillna(0, inplace=True)\n",
    "\n",
    "# Verify that rows with missing values are removed\n",
    "print(data_combined.isnull().sum())\n",
    "\n",
    "# Dropping Duplicate value\n",
    "data_combined = data_combined.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKHz1vC6589G"
   },
   "source": [
    "**Split into Train Data and Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTsP9GFPtW5p",
    "outputId": "22ef2ce2-e7e6-4758-85ed-c312e43f335d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-23 00:00:00\n",
      "Train data shape: (174251, 21)\n",
      "Test data shape: (43560, 21)\n"
     ]
    }
   ],
   "source": [
    "# If the date column is found, retrieve unique dates\n",
    "unique_dates = data_combined['Date'].unique()\n",
    "\n",
    "# Get the date at the 80% position\n",
    "cutoff_date = unique_dates[int(0.8 * len(unique_dates))]\n",
    "print(cutoff_date)\n",
    "\n",
    "# Split the data into train and test sets based on the cutoff_date\n",
    "train_data = data_combined[data_combined['Date'] <= cutoff_date]\n",
    "test_data = data_combined[data_combined['Date'] > cutoff_date]\n",
    "\n",
    "# Print the shapes of the train and test datasets\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8hnaKf5WvBeB"
   },
   "outputs": [],
   "source": [
    "# Prepare data for RNN and LSTM\n",
    "train_data_lstm = train_data.dropna()\n",
    "test_data_lstm = test_data.dropna()\n",
    "train_data_lstm.to_csv('train_data_lstm.csv', index=False)\n",
    "test_data_lstm.to_csv('test_data_lstm.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO1dTaqU6k-s"
   },
   "source": [
    "**Create dataset with lag value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ek7bUA6IwFav"
   },
   "outputs": [],
   "source": [
    "# Sort by 'Symbol' and then by date to maintain chronological order for each stock\n",
    "train_data = train_data.sort_values(by=['Symbol', 'Date'])\n",
    "test_data = test_data.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Create lag features for the past 5 days for each column\n",
    "exclude_columns = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "lag_columns = [col for col in train_data.columns if col not in exclude_columns and (\"news\" in col.lower() or col in ['Close', 'Volume'])]\n",
    "\n",
    "for lag in range(1, 6):\n",
    "    for col in lag_columns:\n",
    "        train_data[f'{col}_lag{lag}'] = train_data.groupby('Symbol')[col].shift(lag)\n",
    "\n",
    "for lag in range(1, 6):\n",
    "    for col in lag_columns:\n",
    "        test_data[f'{col}_lag{lag}'] = test_data[col].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values generated due to lagging\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Save the new train_data to csv files\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index = False)\n",
    "\n",
    "# Filter columns to keep only 'Close' and columns that contain 'lag'\n",
    "filtered_data = train_data[[col for col in train_data.columns if col == 'Close' or 'lag' in col]]\n",
    "filtered_data.to_csv('filtered_data.csv', index = False)\n",
    "filtered_test = test_data[[col for col in test_data.columns if col == 'Close' or 'lag' in col]]\n",
    "filtered_test.to_csv('filtered_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMdiWK7Txc-_"
   },
   "source": [
    "# **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nP20qNMyV8L"
   },
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "\n",
    "X = train_data[feature_columns]\n",
    "y = train_data[target_column]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 1: Feature ranking using L1-LR (Lasso)\n",
    "lasso = Lasso(alpha=0.1, random_state=42)\n",
    "lasso.fit(X_scaled, y)\n",
    "lasso_importance = np.abs(lasso.coef_)\n",
    "lasso_top_features = [feature_columns[i] for i in np.argsort(lasso_importance)[-20:]]\n",
    "\n",
    "# Step 2: Feature ranking using SVM (SVR)\n",
    "svr = SVR(kernel='linear', C=1.0)\n",
    "svr.fit(X_scaled, y)\n",
    "svr_importance = np.abs(svr.coef_.ravel())\n",
    "svr_top_features = [feature_columns[i] for i in np.argsort(svr_importance)[-20:]]\n",
    "\n",
    "# Step 3: Feature ranking using Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs = -1)\n",
    "rf.fit(X_scaled, y)\n",
    "rf_importance = rf.feature_importances_\n",
    "rf_top_features = [feature_columns[i] for i in np.argsort(rf_importance)[-20:]]\n",
    "\n",
    "# Combine top 20 features from each method\n",
    "selected_features = pd.unique(np.concatenate((lasso_top_features, svr_top_features, rf_top_features)))\n",
    "\n",
    "# Step 4: Clustering using Affinity Propagation on selected features\n",
    "X_selected = train_data[selected_features]\n",
    "\n",
    "# Calculate linear correlation matrix\n",
    "correlation_matrix = X_selected.corr().values\n",
    "\n",
    "# Calculate information gain matrix\n",
    "information_gain_matrix = np.zeros((len(selected_features), len(selected_features)))\n",
    "for i in range(len(selected_features)):\n",
    "    for j in range(len(selected_features)):\n",
    "        if i != j:\n",
    "            information_gain_matrix[i, j] = mutual_info_regression(X_selected[[selected_features[i]]], X_selected[selected_features[j]])[0]\n",
    "\n",
    "# Combine correlation and information gain matrices\n",
    "combined_similarity_matrix = (np.abs(correlation_matrix) + information_gain_matrix) / 2\n",
    "\n",
    "# Affinity Propagation clustering\n",
    "affinity_propagation = AffinityPropagation(affinity='precomputed', random_state=42)\n",
    "clusters = affinity_propagation.fit_predict(combined_similarity_matrix)\n",
    "\n",
    "# Assign clusters to features\n",
    "feature_importances = pd.DataFrame({'Feature': selected_features})\n",
    "feature_importances['Cluster'] = clusters\n",
    "\n",
    "# Select exemplar features from each cluster\n",
    "exemplar_features = []\n",
    "for cluster_id in np.unique(clusters):\n",
    "    cluster_features = feature_importances[feature_importances['Cluster'] == cluster_id]['Feature']\n",
    "    exemplar_features.append(cluster_features.iloc[0])\n",
    "\n",
    "# Top features based on Affinity Propagation\n",
    "mffs_top_features = exemplar_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeF_pkO72EKV",
    "outputId": "7d7a16e9-1421-4d00-8fc7-a63b262716e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 11844.9727\n",
      "Epoch 2/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 281.0568\n",
      "Epoch 3/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - loss: 267.0135\n",
      "Epoch 4/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - loss: 218.5582\n",
      "Epoch 5/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 6ms/step - loss: 169.9824\n",
      "Epoch 6/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - loss: 216.9709\n",
      "Epoch 7/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 7ms/step - loss: 186.6322\n",
      "Epoch 8/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 7ms/step - loss: 145.0881\n",
      "Epoch 9/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 7ms/step - loss: 131.5930\n",
      "Epoch 10/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 6ms/step - loss: 131.9636\n",
      "Epoch 11/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 8ms/step - loss: 124.2244\n",
      "Epoch 12/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 6ms/step - loss: 106.2337\n",
      "Epoch 13/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 7ms/step - loss: 126.5436\n",
      "Epoch 14/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 8ms/step - loss: 109.5891\n",
      "Epoch 15/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 6ms/step - loss: 97.7820\n",
      "Epoch 16/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 7ms/step - loss: 101.1344\n",
      "Epoch 17/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 6ms/step - loss: 98.8960\n",
      "Epoch 18/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - loss: 88.9447\n",
      "Epoch 19/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 6ms/step - loss: 94.3368\n",
      "Epoch 20/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - loss: 95.0731\n",
      "Epoch 21/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 5ms/step - loss: 100.2786\n",
      "Epoch 22/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - loss: 95.6423\n",
      "Epoch 23/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - loss: 83.5894\n",
      "Epoch 24/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 6ms/step - loss: 88.8664\n",
      "Epoch 25/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 78.8710\n",
      "Epoch 26/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 8ms/step - loss: 82.6834\n",
      "Epoch 27/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 7ms/step - loss: 86.3740\n",
      "Epoch 28/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 7ms/step - loss: 83.2595\n",
      "Epoch 29/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - loss: 75.8708\n",
      "Epoch 30/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 7ms/step - loss: 78.1069\n",
      "Epoch 31/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 8ms/step - loss: 80.5090\n",
      "Epoch 32/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - loss: 80.3458\n",
      "Epoch 33/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 80.9706\n",
      "Epoch 34/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 75.7672\n",
      "Epoch 35/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 8ms/step - loss: 85.1546\n",
      "Epoch 36/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 6ms/step - loss: 74.5464\n",
      "Epoch 37/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 8ms/step - loss: 75.9715\n",
      "Epoch 38/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 5ms/step - loss: 77.5802\n",
      "Epoch 39/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 7ms/step - loss: 77.7923\n",
      "Epoch 40/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 6ms/step - loss: 70.6299\n",
      "Epoch 41/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 80.9765\n",
      "Epoch 42/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 79.8322\n",
      "Epoch 43/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 77.8456\n",
      "Epoch 44/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 6ms/step - loss: 71.1270\n",
      "Epoch 45/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 6ms/step - loss: 71.4888\n",
      "Epoch 46/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 6ms/step - loss: 86.5908\n",
      "Epoch 47/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 74.6537\n",
      "Epoch 48/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - loss: 73.2370\n",
      "Epoch 49/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 6ms/step - loss: 72.2716\n",
      "Epoch 50/50\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 6ms/step - loss: 71.3965\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 79447.8594\n",
      "Epoch 2/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 2191.8862\n",
      "Epoch 3/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 531.6158\n",
      "Epoch 4/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1ms/step - loss: 191.3537\n",
      "Epoch 5/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 99.4176\n",
      "Epoch 6/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 69.6757\n",
      "Epoch 7/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 56.8737\n",
      "Epoch 8/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 50.4129\n",
      "Epoch 9/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 45.1525\n",
      "Epoch 10/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 42.2870\n",
      "Epoch 11/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 43.8311\n",
      "Epoch 12/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 40.7157\n",
      "Epoch 13/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 37.1618\n",
      "Epoch 14/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 36.7662\n",
      "Epoch 15/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 33.9609\n",
      "Epoch 16/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 33.6763\n",
      "Epoch 17/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 31.0535\n",
      "Epoch 18/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 30.7186\n",
      "Epoch 19/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 29.5161\n",
      "Epoch 20/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 29.6172\n",
      "Epoch 21/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 27.2120\n",
      "Epoch 22/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 27.7976\n",
      "Epoch 23/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 25.6944\n",
      "Epoch 24/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 25.0949\n",
      "Epoch 25/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 23.3563\n",
      "Epoch 26/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 22.3735\n",
      "Epoch 27/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 21.5855\n",
      "Epoch 28/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 22.5926\n",
      "Epoch 29/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 21.0822\n",
      "Epoch 30/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 21.3323\n",
      "Epoch 31/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 19.7281\n",
      "Epoch 32/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 20.3504\n",
      "Epoch 33/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 19.1450\n",
      "Epoch 34/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 18.2061\n",
      "Epoch 35/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 17.5027\n",
      "Epoch 36/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 18.6504\n",
      "Epoch 37/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 17.1897\n",
      "Epoch 38/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 18.0051\n",
      "Epoch 39/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 16.7306\n",
      "Epoch 40/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 15.6466\n",
      "Epoch 41/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 15.7786\n",
      "Epoch 42/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 16.7208\n",
      "Epoch 43/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 16.9731\n",
      "Epoch 44/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 16.5551\n",
      "Epoch 45/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 15.6464\n",
      "Epoch 46/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.5739\n",
      "Epoch 47/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 14.9401\n",
      "Epoch 48/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 14.9609\n",
      "Epoch 49/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 15.2086\n",
      "Epoch 50/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 15.2080\n",
      "Epoch 51/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 15.4287\n",
      "Epoch 52/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 16.0635\n",
      "Epoch 53/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 14.8120\n",
      "Epoch 54/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 14.9796\n",
      "Epoch 55/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 14.1648\n",
      "Epoch 56/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 15.2703\n",
      "Epoch 57/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 13.4591\n",
      "Epoch 58/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.5142\n",
      "Epoch 59/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 15.9311\n",
      "Epoch 60/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 15.2071\n",
      "Epoch 61/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 15.2467\n",
      "Epoch 62/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 14.2115\n",
      "Epoch 63/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 14.3665\n",
      "Epoch 64/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.5317\n",
      "Epoch 65/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 14.2391\n",
      "Epoch 66/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 14.3041\n",
      "Epoch 67/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 13.8736\n",
      "Epoch 68/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 14.2530\n",
      "Epoch 69/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.2388\n",
      "Epoch 70/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 13.9971\n",
      "Epoch 71/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 14.2817\n",
      "Epoch 72/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.2352\n",
      "Epoch 73/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 13.5470\n",
      "Epoch 74/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 14.4900\n",
      "Epoch 75/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 14.7960\n",
      "Epoch 76/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 13.9542\n",
      "Epoch 77/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 14.3346\n",
      "Epoch 78/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 13.8383\n",
      "Epoch 79/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 15.0350\n",
      "Epoch 80/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 13.8688\n",
      "Epoch 81/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 13.6908\n",
      "Epoch 82/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.4671\n",
      "Epoch 83/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 14.0794\n",
      "Epoch 84/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 14.1384\n",
      "Epoch 85/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 12.8631\n",
      "Epoch 86/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 13.9310\n",
      "Epoch 87/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 14.5253\n",
      "Epoch 88/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.9321\n",
      "Epoch 89/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 13.8833\n",
      "Epoch 90/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 14.0975\n",
      "Epoch 91/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.8250\n",
      "Epoch 92/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 13.8054\n",
      "Epoch 93/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 13.6151\n",
      "Epoch 94/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 12.4668\n",
      "Epoch 95/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 12.5499\n",
      "Epoch 96/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 13.3660\n",
      "Epoch 97/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 14.4578\n",
      "Epoch 98/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 12.9814\n",
      "Epoch 99/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 13.6227\n",
      "Epoch 100/100\n",
      "\u001b[1m5368/5368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 14.0482\n",
      "Top selected features: Index(['Volume_lag4', 'News - Analyst Comments_lag3',\n",
      "       'News - Analyst Comments_lag2', 'News - Stock Rumors_lag4',\n",
      "       'News - Stock Rumors_lag1', 'News - Stocks_lag1', 'News - Stocks_lag4',\n",
      "       'News - Stocks_lag5', 'News - Analyst Comments_lag5',\n",
      "       'News - Corporate Earnings_lag1', 'News - Stock Rumors_lag2',\n",
      "       'News - Stock Rumors_lag3', 'News - Layoffs_lag4', 'Volume_lag1',\n",
      "       'News - Stock Rumors_lag5', 'Close_lag4', 'Close_lag5', 'Close_lag3',\n",
      "       'Close_lag2', 'Close_lag1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Define the Multi-Filters Neural Network (MFNN) as the teacher model with three distinct paths\n",
    "class MFNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MFNN, self).__init__()\n",
    "\n",
    "        # Path 1: Single Convolutional Layer\n",
    "        self.single_conv = Conv1D(64, kernel_size=3, activation='relu', padding='same')\n",
    "\n",
    "        # Path 2: Two Convolutional Layers\n",
    "        self.double_conv1 = Conv1D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.double_conv2 = Conv1D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.batch_norm_conv = BatchNormalization()\n",
    "\n",
    "        # Path 3: Recurrent Pathway (GRU)\n",
    "        self.gru = GRU(64, return_sequences=True)\n",
    "        self.batch_norm_rnn = BatchNormalization()\n",
    "\n",
    "        # Dense layers for final prediction\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(128, activation='relu')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.fc2 = Dense(2)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Path 1: Single Convolutional Layer\n",
    "        x_single_conv = self.single_conv(x)\n",
    "\n",
    "        # Path 2: Two Convolutional Layers\n",
    "        x_double_conv = self.double_conv1(x)\n",
    "        x_double_conv = self.double_conv2(x_double_conv)\n",
    "        x_double_conv = self.batch_norm_conv(x_double_conv)\n",
    "\n",
    "        # Path 3: Recurrent pathway\n",
    "        x_rnn = self.gru(x)\n",
    "        x_rnn = self.batch_norm_rnn(x_rnn)\n",
    "\n",
    "        # Concatenate outputs from all paths\n",
    "        x_combined = tf.concat([\n",
    "            tf.reduce_mean(x_single_conv, axis=1),\n",
    "            tf.reduce_mean(x_double_conv, axis=1),\n",
    "            tf.reduce_mean(x_rnn, axis=1)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Final Dense layers\n",
    "        x = self.flatten(x_combined)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Reshape input data for the MFNN (expected 3D input: samples, timesteps, features)\n",
    "X_scaled_expanded = np.expand_dims(X_scaled, axis=1)\n",
    "\n",
    "# Initialize and compile the MFNN model\n",
    "teacher_model = MFNN(input_shape=(X_scaled_expanded.shape[1], X_scaled_expanded.shape[2]))\n",
    "teacher_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the teacher model on the full feature set to learn the representation\n",
    "teacher_model.fit(X_scaled_expanded, y, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Use the MFNN model to generate 2D feature representations for the student\n",
    "X_teacher_representation = teacher_model.predict(X_scaled_expanded)\n",
    "\n",
    "# Define the student network to learn the MFNN feature representation\n",
    "student_model = Sequential([\n",
    "    Dense(20, activation='relu', input_shape=(X.shape[1],)),\n",
    "    Dense(2)\n",
    "])\n",
    "\n",
    "# Compile the student model\n",
    "student_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the student model to learn from the MFNN representation\n",
    "student_model.fit(X_scaled, X_teacher_representation, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Extract feature importance from the student model's first layer weights\n",
    "# Apply row-sparsity constraint to the weight matrix of the first layer\n",
    "W1 = student_model.layers[0].get_weights()[0]\n",
    "feature_importance = np.mean(np.abs(W1), axis=1)\n",
    "\n",
    "# Select the top features based on feature importance scores\n",
    "top_features_idx = np.argsort(feature_importance)[-20:]\n",
    "selected_features = X.columns[top_features_idx]\n",
    "\n",
    "print(\"Top selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-U4Zupr9Q9V"
   },
   "source": [
    "# **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3SVZlaB_qse"
   },
   "source": [
    "**ANN using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JO2_Q74zAAwv"
   },
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert numpy arrays to DataFrames\n",
    "X = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "y = y_train\n",
    "\n",
    "# Expanding window time series validation and hyperparameter tuning\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the model function\n",
    "def create_model(layers=3, neurons=16, activation='relu', learning_rate=0.005, dropout_rate=0.1, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    for _ in range(layers):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'layers': [3, 5, 7],\n",
    "    'neurons': [16, 32, 64],\n",
    "    'activation': ['relu', 'sigmoid'],\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'dropout_rate': [0.1, 0.5],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [20, 50]\n",
    "}\n",
    "\n",
    "# Sample 20 random combinations\n",
    "random.seed(42)\n",
    "param_combinations = list(ParameterSampler(param_grid, n_iter=20, random_state=42))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate each combination\n",
    "for params in param_combinations:\n",
    "    epochs = params.pop('epochs')\n",
    "    batch_size = params.pop('batch_size')\n",
    "\n",
    "    # Initialize a list to hold MSE for each split\n",
    "    mse_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Create and train the model\n",
    "        model = create_model(**params)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "\n",
    "        # Check for NaN values in y_val and y_pred\n",
    "        if np.isnan(y_val).any() or np.isnan(y_pred).any():\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # Calculate average MSE if there are valid mse values\n",
    "    if mse_list:\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        results.append((avg_mse, {**params, 'epochs': epochs, 'batch_size': batch_size}))\n",
    "\n",
    "# Sort results by average MSE and get the top 3 combinations\n",
    "results.sort(key=lambda x: x[0])\n",
    "top_3 = results[:3]\n",
    "\n",
    "# Display the top 3 combinations with the lowest average MSE\n",
    "print(\"Top 3 hyperparameter combinations based on average MSE:\")\n",
    "for i, (avg_mse, params) in enumerate(top_3, start=1):\n",
    "    print(f\"Rank {i}: MSE = {avg_mse:.4f}, Parameters = {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkXVnkMUG00H"
   },
   "source": [
    "**ANN using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm3PpomCG8Z7"
   },
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert numpy arrays to DataFrames\n",
    "X = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "y = y_train\n",
    "\n",
    "# Expanding window time series validation and hyperparameter tuning\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the model function with parameters as trial suggestions\n",
    "def create_model(trial):\n",
    "    layers = trial.suggest_int(\"layers\", 1, 7)\n",
    "    neurons = trial.suggest_categorical(\"neurons\", [16, 32, 64])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\"])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "\n",
    "    model = Sequential()\n",
    "    for _ in range(layers):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Objective function to minimize MSE using Optuna\n",
    "def objective(trial):\n",
    "    # Extract hyperparameters for batch_size and epochs\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    epochs = trial.suggest_int(\"epochs\", 20, 50)\n",
    "\n",
    "    mse_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Create and train the model\n",
    "        model = create_model(trial)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "\n",
    "        # Skip if NaN values are detected\n",
    "        if np.isnan(y_val).any() or np.isnan(y_pred).any():\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # Return the mean MSE across splits, or a high MSE if no valid splits\n",
    "    return np.mean(mse_list) if mse_list else float(\"inf\")\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Display the best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZfPk40w_5ZP"
   },
   "source": [
    "**GRU using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgDJHYgLABcF"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = filtered_data\n",
    "\n",
    "# Define target column and exclude non-numeric columns\n",
    "target_col = 'Close'\n",
    "non_numeric_cols = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "feature_cols = [col for col in data.columns if col not in non_numeric_cols and col != target_col]\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale features and target column\n",
    "data[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "data[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "\n",
    "# Prepare data using previous 5 days' features only, excluding today's data\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "for symbol in data['Symbol'].unique():\n",
    "    company_data = data[data['Symbol'] == symbol].reset_index(drop=True)\n",
    "\n",
    "    for i in range(sequence_length, len(company_data)):\n",
    "        # Use the previous 5 days as input features, excluding today\n",
    "        X.append(company_data[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        # Target is the 'Close' price of the current day\n",
    "        y.append(company_data[target_col].iloc[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Define the GRU model function\n",
    "def create_gru_model(layers=3, hidden_units=50, dropout_rate=0.2, learning_rate=0.005, optimizer='adam', grad_clip=1):\n",
    "    model = Sequential()\n",
    "    for layer in range(layers):\n",
    "        return_sequences = layer < (layers - 1)\n",
    "        model.add(GRU(units=hidden_units, return_sequences=return_sequences))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer_instance = Adam(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer_instance = RMSprop(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "\n",
    "    model.compile(optimizer=optimizer_instance, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'layers': [1, 2, 3],\n",
    "    'hidden_units': [75, 100],\n",
    "    'learning_rate': [0.005, 0.01, 0.02],\n",
    "    'dropout_rate': [0.2, 0.4],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10, 50],\n",
    "    'grad_clip': [1, 5]\n",
    "}\n",
    "\n",
    "# Sample 20 random combinations\n",
    "random.seed(42)\n",
    "param_combinations = list(ParameterSampler(param_grid, n_iter=20, random_state=42))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate each combination\n",
    "for params in param_combinations:\n",
    "    epochs = params.pop('epochs')\n",
    "    batch_size = params.pop('batch_size')\n",
    "\n",
    "    # Initialize a list to hold MSE for each split\n",
    "    mse_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Create and train the model\n",
    "        model = create_gru_model(**params)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "\n",
    "        # Reverse scaling for validation and prediction\n",
    "        y_val_true = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "        y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "        # Check for NaN values in y_val and y_pred\n",
    "        if np.isnan(y_val_true).any() or np.isnan(y_pred).any():\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_val_true, y_pred)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # Calculate average MSE if there are valid mse values\n",
    "    if mse_list:\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        results.append((avg_mse, {**params, 'epochs': epochs, 'batch_size': batch_size}))\n",
    "\n",
    "# Sort results by average MSE and get the top 3 combinations\n",
    "results.sort(key=lambda x: x[0])\n",
    "top_3 = results[:3]\n",
    "\n",
    "# Display the top 3 combinations with the lowest average MSE\n",
    "print(\"Top 3 hyperparameter combinations based on average MSE:\")\n",
    "for i, (avg_mse, params) in enumerate(top_3, start=1):\n",
    "    print(f\"Rank {i}: MSE = {avg_mse:.4f}, Parameters = {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRm0-dLsHB3A"
   },
   "source": [
    "**GRU using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25V6RuCdHSSX"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = filtered_data\n",
    "target_col = 'Close'\n",
    "non_numeric_cols = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "feature_cols = [col for col in data.columns if col not in non_numeric_cols and col != target_col]\n",
    "\n",
    "# Initialize scaler for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale data\n",
    "data[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "data[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "\n",
    "# Define sequence length for GRU\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "# Prepare data by company, ensuring sequence consistency\n",
    "for symbol in data['Symbol'].unique():\n",
    "    company_data = data[data['Symbol'] == symbol]\n",
    "    for i in range(len(company_data) - sequence_length):\n",
    "        X.append(company_data[feature_cols].iloc[i:i + sequence_length].values)\n",
    "        y.append(company_data[target_col].iloc[i + sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    layers = trial.suggest_int('layers', 1, 3)\n",
    "    hidden_units = trial.suggest_int('hidden_units', 50, 200)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.1)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "    grad_clip = trial.suggest_int('grad_clip', 1, 5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 10, 50)\n",
    "\n",
    "    # Initialize list to store MSE for each fold\n",
    "    mse_scores = []\n",
    "\n",
    "    # Expanding window cross-validation\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Build GRU model\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "        for i in range(layers):\n",
    "            return_sequences = i < layers - 1\n",
    "            model.add(GRU(hidden_units, return_sequences=return_sequences, kernel_constraint=MaxNorm(grad_clip)))\n",
    "            if dropout_rate > 0:\n",
    "                model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "        else:\n",
    "            optimizer = RMSprop(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "        # Train model on training split\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val),\n",
    "                  callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE for validation data\n",
    "        y_val_pred_scaled = model.predict(X_val)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)\n",
    "        y_val_true = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        mse = mean_squared_error(y_val_true, y_val_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Return the average MSE across all folds\n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    return avg_mse\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChFLjGHz_wMl"
   },
   "source": [
    "**RNN using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OccqE6hoACDq"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = filtered_data\n",
    "\n",
    "# Define target column and exclude non-numeric columns\n",
    "target_col = 'Close'\n",
    "non_numeric_cols = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "feature_cols = [col for col in data.columns if col not in non_numeric_cols and col != target_col]\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale features and target column\n",
    "data[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "data[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "\n",
    "# Prepare data using previous 5 days' features only, excluding today's data\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "for symbol in data['Symbol'].unique():\n",
    "    company_data = data[data['Symbol'] == symbol].reset_index(drop=True)\n",
    "\n",
    "    for i in range(sequence_length, len(company_data)):\n",
    "        # Use the previous 5 days as input features, excluding today\n",
    "        X.append(company_data[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        # Target is the 'Close' price of the current day\n",
    "        y.append(company_data[target_col].iloc[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Define the RNN model function\n",
    "def create_rnn_model(layers=3, hidden_units=50, dropout_rate=0.2, learning_rate=0.005, optimizer='adam', grad_clip=1):\n",
    "    model = Sequential()\n",
    "    for layer in range(layers):\n",
    "        return_sequences = layer < (layers - 1)\n",
    "        model.add(SimpleRNN(units=hidden_units, return_sequences=return_sequences))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer_instance = Adam(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer_instance = RMSprop(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "\n",
    "    model.compile(optimizer=optimizer_instance, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'layers': [1, 2, 3],\n",
    "    'hidden_units': [50, 75, 100],\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [20, 50],\n",
    "    'grad_clip': [1, 5],\n",
    "    'dropout_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Sample 20 random combinations\n",
    "random.seed(42)\n",
    "param_combinations = list(ParameterSampler(param_grid, n_iter=20, random_state=42))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate each combination\n",
    "for params in param_combinations:\n",
    "    epochs = params.pop('epochs')\n",
    "    batch_size = params.pop('batch_size')\n",
    "\n",
    "    # Initialize a list to hold MSE for each split\n",
    "    mse_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Create and train the model\n",
    "        model = create_rnn_model(**params)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "\n",
    "        # Reverse scaling for validation and prediction\n",
    "        y_val_true = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "        y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "        # Check for NaN values in y_val and y_pred\n",
    "        if np.isnan(y_val_true).any() or np.isnan(y_pred).any():\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_val_true, y_pred)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # Calculate average MSE if there are valid mse values\n",
    "    if mse_list:\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        results.append((avg_mse, {**params, 'epochs': epochs, 'batch_size': batch_size}))\n",
    "\n",
    "# Sort results by average MSE and get the top 3 combinations\n",
    "results.sort(key=lambda x: x[0])\n",
    "top_3 = results[:3]\n",
    "\n",
    "# Display the top 3 combinations with the lowest average MSE\n",
    "print(\"Top 3 hyperparameter combinations based on average MSE:\")\n",
    "for i, (avg_mse, params) in enumerate(top_3, start=1):\n",
    "    print(f\"Rank {i}: MSE = {avg_mse:.4f}, Parameters = {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MyKoXCuHS-E"
   },
   "source": [
    "**RNN using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc_pTICZHXPz"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = filtered_data\n",
    "target_col = 'Close'\n",
    "non_numeric_cols = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "feature_cols = [col for col in data.columns if col not in non_numeric_cols and col != target_col]\n",
    "\n",
    "# Initialize scaler for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale data\n",
    "data[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "data[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "\n",
    "# Define sequence length for RNN\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "# Prepare data by company, keeping sequence consistency\n",
    "for symbol in data['Symbol'].unique():\n",
    "    company_data = data[data['Symbol'] == symbol]\n",
    "    for i in range(len(company_data) - sequence_length):\n",
    "        X.append(company_data[feature_cols].iloc[i:i + sequence_length].values)\n",
    "        y.append(company_data[target_col].iloc[i + sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    layers = trial.suggest_int('layers', 1, 3)\n",
    "    hidden_units = trial.suggest_int('hidden_units', 50, 150)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.1)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "    grad_clip = trial.suggest_int('grad_clip', 1, 5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "\n",
    "    # Initialize list to store MSE for each fold\n",
    "    mse_scores = []\n",
    "\n",
    "    # Expanding window cross-validation\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Build RNN model\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "        for i in range(layers):\n",
    "            return_sequences = i < layers - 1\n",
    "            model.add(SimpleRNN(hidden_units, return_sequences=return_sequences, kernel_constraint=MaxNorm(grad_clip)))\n",
    "            if dropout_rate > 0:\n",
    "                model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "        else:\n",
    "            optimizer = RMSprop(learning_rate=learning_rate, clipvalue=grad_clip)\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Train model on training split\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val),\n",
    "                  callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE for validation data\n",
    "        y_val_pred_scaled = model.predict(X_val)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)\n",
    "        y_val_true = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        mse = mean_squared_error(y_val_true, y_val_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Return the average MSE across all folds\n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    return avg_mse\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG8nNjTEE0KE"
   },
   "source": [
    "**LSTM using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlPfS0FmE4oq"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = filtered_data\n",
    "\n",
    "# Define target column and exclude non-numeric columns\n",
    "target_col = 'Close'\n",
    "non_numeric_cols = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "feature_cols = [col for col in data.columns if col not in non_numeric_cols and col != target_col]\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale features and target column\n",
    "data[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "data[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "\n",
    "# Prepare data using previous 5 days' features only, excluding today's data\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "for symbol in data['Symbol'].unique():\n",
    "    company_data = data[data['Symbol'] == symbol].reset_index(drop=True)\n",
    "\n",
    "    for i in range(sequence_length, len(company_data)):\n",
    "        # Use the previous 5 days as input features, excluding today\n",
    "        X.append(company_data[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        # Target is the 'Close' price of the current day\n",
    "        y.append(company_data[target_col].iloc[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Define the model function\n",
    "def create_lstm_model(layers=3, hidden_units=50, dropout_rate=0.2, learning_rate=0.005, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    for layer in range(layers):\n",
    "        return_sequences = layer < (layers - 1)\n",
    "        model.add(LSTM(units=hidden_units, return_sequences=return_sequences))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer_instance = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer_instance = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer_instance, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'layers': [3, 5, 7],\n",
    "    'hidden_units': [50, 75, 100],\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'dropout_rate': [0.2, 0.4],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [20, 50]\n",
    "}\n",
    "\n",
    "# Sample 20 random combinations\n",
    "random.seed(42)\n",
    "param_combinations = list(ParameterSampler(param_grid, n_iter=20, random_state=42))\n",
    "\n",
    "# TimeSeriesSplit for expanding window cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate each combination\n",
    "for params in param_combinations:\n",
    "    epochs = params.pop('epochs')\n",
    "    batch_size = params.pop('batch_size')\n",
    "\n",
    "    # Initialize a list to hold MSE for each split\n",
    "    mse_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Create and train the model\n",
    "        model = create_lstm_model(**params)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "\n",
    "        # Reverse scaling for validation and prediction\n",
    "        y_val_true = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "        y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "        # Check for NaN values in y_val and y_pred\n",
    "        if np.isnan(y_val_true).any() or np.isnan(y_pred).any():\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_val_true, y_pred)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # Calculate average MSE if there are valid mse values\n",
    "    if mse_list:\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        results.append((avg_mse, {**params, 'epochs': epochs, 'batch_size': batch_size}))\n",
    "\n",
    "# Sort results by average MSE and get the top 3 combinations\n",
    "results.sort(key=lambda x: x[0])\n",
    "top_3 = results[:3]\n",
    "\n",
    "# Display the top 3 combinations with the lowest average MSE\n",
    "print(\"Top 3 hyperparameter combinations based on average MSE:\")\n",
    "for i, (avg_mse, params) in enumerate(top_3, start=1):\n",
    "    print(f\"Rank {i}: MSE = {avg_mse:.4f}, Parameters = {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RjUQ73BAC4g"
   },
   "source": [
    "**LSTM using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQdk6qSUAIsw"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = filtered_data\n",
    "\n",
    "# Define target column and exclude non-numeric columns\n",
    "target_col = 'Close'\n",
    "non_numeric_cols = ['Symbol', 'GICS Sector', 'Headquarters Location', 'Founded', 'Date']\n",
    "feature_cols = [col for col in data.columns if col not in non_numeric_cols and col != target_col]\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale features and target column\n",
    "data[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "data[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "\n",
    "# Prepare data using previous 5 days' features only, excluding today's data\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "for symbol in data['Symbol'].unique():\n",
    "    company_data = data[data['Symbol'] == symbol].reset_index(drop=True)\n",
    "\n",
    "    for i in range(sequence_length, len(company_data)):\n",
    "        # Use the previous 5 days as input features, excluding today\n",
    "        X.append(company_data[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        # Target is the 'Close' price of the current day\n",
    "        y.append(company_data[target_col].iloc[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Define the objective function for Optuna with expanding window validation\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    n_units = trial.suggest_int('n_units', 50, 200)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 10, 50)\n",
    "\n",
    "    # Expanding window split for time series\n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    val_mse_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Build LSTM model with a variable number of LSTM layers\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "        for layer in range(n_layers):\n",
    "            # Add LSTM layers as per the suggested number of layers\n",
    "            return_sequences = layer < (n_layers - 1)  # Only set `return_sequences=True` for all but the last layer\n",
    "            model.add(LSTM(units=n_units, return_sequences=return_sequences))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "        # Dense output layer\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "        # Early stopping and pruning callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "        pruning_callback = KerasPruningCallback(trial, monitor=\"val_loss\")\n",
    "\n",
    "        # Train model with pruning callback\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                  validation_data=(X_val, y_val), callbacks=[early_stopping, pruning_callback], verbose=0)\n",
    "\n",
    "        # Predict on the validation set and calculate MSE\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_true = scaler_y.inverse_transform(y_val.reshape(-1, 1))  # Reverse scaling\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred)  # Reverse scaling\n",
    "\n",
    "        val_mse = mean_squared_error(y_val_true, y_val_pred)\n",
    "        val_mse_scores.append(val_mse)\n",
    "\n",
    "    # Return average MSE across the expanding window splits\n",
    "    avg_val_mse = np.mean(val_mse_scores)\n",
    "    return avg_val_mse\n",
    "\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, n_jobs = -1)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best MSE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-4nkIGkAJlg"
   },
   "source": [
    "**CatBoost using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQYUKPdpANSM"
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid for CatBoost\n",
    "param_grid_catboost = {\n",
    "    'iterations': [50, 100, 150],\n",
    "    'depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'border_count': [32, 64, 128],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Sample 50 random combinations of hyperparameters\n",
    "param_combinations_catboost = list(ParameterSampler(param_grid_catboost, n_iter=50, random_state=42))\n",
    "\n",
    "# Store the best parameters and the lowest MSE\n",
    "best_params_catboost = None\n",
    "lowest_mse_catboost = float(\"inf\")\n",
    "\n",
    "# Hyperparameter tuning on the whole training set with all features for CatBoost\n",
    "for params in param_combinations_catboost:\n",
    "    mse_scores = []  # To track MSE scores across splits\n",
    "\n",
    "    # Expanding window cross-validation\n",
    "    for train_idx, val_idx in tscv.split(X_all):\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Initialize the model with current hyperparameters\n",
    "        model = CatBoostRegressor(**params, random_state=42, silent=True, thread_count=-1)\n",
    "\n",
    "        # Train and validate on the current split\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Calculate the average MSE across all splits\n",
    "    avg_mse = np.mean(mse_scores)\n",
    "\n",
    "    # Update the best parameters if the current average MSE is lower\n",
    "    if avg_mse < lowest_mse_catboost:\n",
    "        lowest_mse_catboost = avg_mse\n",
    "        best_params_catboost = params\n",
    "\n",
    "print(f\"Best hyperparameters found for CatBoost: {best_params_catboost}\")\n",
    "print(f\"Lowest MSE for CatBoost on full feature set: {lowest_mse_catboost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO2Cxd-LHgH6"
   },
   "source": [
    "**CatBoost using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHfEfFkjHnVd"
   },
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "# Expanding window time series validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be tuned for CatBoost\n",
    "    param = {\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'random_strength': trial.suggest_int('random_strength', 1, 20),\n",
    "        'random_seed': trial.suggest_int('random_seed', 1, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1.0, 10.0),\n",
    "        'iterations': trial.suggest_categorical('iterations', [100, 200, 500, 1000]),\n",
    "        'depth': trial.suggest_int('depth', 3, 12),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0)\n",
    "    }\n",
    "\n",
    "    # List to store MSE for each fold\n",
    "    mse_list = []\n",
    "\n",
    "    # Perform time series cross-validation\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Fit model with the current parameter combination\n",
    "        model = CatBoostRegressor(**param, verbose=0)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the test fold\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "\n",
    "        # Calculate MSE for the fold\n",
    "        mse_fold = mean_squared_error(y_test_fold, y_pred_fold)\n",
    "        mse_list.append(mse_fold)\n",
    "\n",
    "    # Return the average MSE across all folds\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "# Run Optuna to find the best hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get the best parameters and best MSE from the study\n",
    "best_params = study.best_params\n",
    "best_mse = study.best_value\n",
    "\n",
    "# Output the best parameter combination and average MSE\n",
    "print(\"Best parameter combination:\", best_params)\n",
    "print(\"Best average MSE:\", best_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm3n1LFEANn6"
   },
   "source": [
    "**XGBoost using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79OZP0N0Aeah"
   },
   "outputs": [],
   "source": [
    "# Define TimeSeriesSplit for expanding window cross-validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "\n",
    "X_train_data = train_data[feature_columns]\n",
    "y_train_data = train_data[target_column]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_data)\n",
    "\n",
    "# Convert numpy arrays to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_data.columns)\n",
    "\n",
    "# Assuming 'train_data' is already loaded and scaled\n",
    "X_all = X_train_scaled_df.values\n",
    "y = train_data['Close'].values\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Sample 50 random combinations of hyperparameters\n",
    "param_combinations_xgb = list(ParameterSampler(param_grid_xgb, n_iter=50, random_state=42))\n",
    "\n",
    "# Store the best parameters and the lowest MSE\n",
    "best_params_xgb = None\n",
    "lowest_mse_xgb = float(\"inf\")\n",
    "\n",
    "# Hyperparameter tuning on the whole training set with all features for XGBoost\n",
    "for params in param_combinations_xgb:\n",
    "    mse_scores = []  # To track MSE scores across splits\n",
    "\n",
    "    # Expanding window cross-validation\n",
    "    for train_idx, val_idx in tscv.split(X_all):\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Initialize the model with current hyperparameters\n",
    "        model = XGBRegressor(**params, random_state=42, n_jobs=-1)\n",
    "\n",
    "        # Train and validate on the current split\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Calculate the average MSE across all splits\n",
    "    avg_mse = np.mean(mse_scores)\n",
    "\n",
    "    # Update the best parameters if the current average MSE is lower\n",
    "    if avg_mse < lowest_mse_xgb:\n",
    "        lowest_mse_xgb = avg_mse\n",
    "        best_params_xgb = params\n",
    "\n",
    "print(f\"Best hyperparameters found for XGBoost: {best_params_xgb}\")\n",
    "print(f\"Lowest MSE for XGBoost on full feature set: {lowest_mse_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUylmxAFHsN_"
   },
   "source": [
    "**XGBoost using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK5LiZfvH_3l"
   },
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "# Expanding window time series validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be tuned\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.5, step=0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 200, 500, 1000]),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 0.01, 1.0),\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1.0, 5.0)\n",
    "    }\n",
    "\n",
    "    # List to store MSE for each fold\n",
    "    mse_list = []\n",
    "\n",
    "    # Perform time series cross-validation\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Fit model with the current parameter combination\n",
    "        model = xgb.XGBRegressor(**param, n_jobs=-1)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the test fold\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "\n",
    "        # Calculate MSE for the fold\n",
    "        mse_fold = mean_squared_error(y_test_fold, y_pred_fold)\n",
    "        mse_list.append(mse_fold)\n",
    "\n",
    "    # Return the average MSE across all folds\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "# Run Optuna to find the best hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get the best parameters and best MSE from the study\n",
    "best_params = study.best_params\n",
    "best_mse = study.best_value\n",
    "\n",
    "# Output the best parameter combination and average MSE\n",
    "print(\"Best parameter combination:\", best_params)\n",
    "print(\"Best average MSE:\", best_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXPgLMZkAeuE"
   },
   "source": [
    "**Random Forest using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyTTZbbfAhr4"
   },
   "outputs": [],
   "source": [
    "# Define TimeSeriesSplit for expanding window cross-validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "\n",
    "X_train_data = train_data[feature_columns]\n",
    "y_train_data = train_data[target_column]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_data)\n",
    "\n",
    "# Convert numpy arrays to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_data.columns)\n",
    "\n",
    "# Assuming 'train_data' is already loaded and scaled\n",
    "X_all = X_train_scaled_df.values\n",
    "y = train_data['Close'].values\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Sample 50 random combinations of hyperparameters\n",
    "param_combinations = list(ParameterSampler(param_grid, n_iter=50, random_state=42))\n",
    "\n",
    "# Store the best parameters and the lowest MSE\n",
    "best_params = None\n",
    "lowest_mse = float(\"inf\")\n",
    "\n",
    "# Hyperparameter tuning on the whole training set with all features\n",
    "for params in param_combinations:\n",
    "    mse_scores = []  # To track MSE scores across splits\n",
    "\n",
    "    # Expanding window cross-validation\n",
    "    for train_idx, val_idx in tscv.split(X_all):\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Initialize the model with current hyperparameters\n",
    "        model = RandomForestRegressor(**params, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # Train and validate on the current split\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Calculate the average MSE across all splits\n",
    "    avg_mse = np.mean(mse_scores)\n",
    "\n",
    "    # Update the best parameters if the current average MSE is lower\n",
    "    if avg_mse < lowest_mse:\n",
    "        lowest_mse = avg_mse\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best hyperparameters found on the full feature set: {best_params}\")\n",
    "print(f\"Lowest MSE on full feature set: {lowest_mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvY9cEXSIAgd"
   },
   "source": [
    "**Random Forest using Optuna**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcAoHT-hIHKg"
   },
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if 'lag' in col]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "# Expanding window time series validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be tuned for Random Forest\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None,'sqrt', 'log2']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "\n",
    "    # List to store MSE for each fold\n",
    "    mse_list = []\n",
    "\n",
    "    # Perform time series cross-validation\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Fit model with the current parameter combination\n",
    "        model = RandomForestRegressor(**param, n_jobs=-1, random_state=42)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the test fold\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "\n",
    "        # Calculate MSE for the fold\n",
    "        mse_fold = mean_squared_error(y_test_fold, y_pred_fold)\n",
    "        mse_list.append(mse_fold)\n",
    "\n",
    "    # Return the average MSE across all folds\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "# Run Optuna to find the best hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get the best parameters and best MSE from the study\n",
    "best_params = study.best_params\n",
    "best_mse = study.best_value\n",
    "\n",
    "# Output the best parameter combination and average MSE\n",
    "print(\"Best parameter combination:\", best_params)\n",
    "print(\"Best average MSE:\", best_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkMojDJApuQi"
   },
   "source": [
    "**Decision Tree using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cudjUrCrqDTW"
   },
   "outputs": [],
   "source": [
    "# Define TimeSeriesSplit for expanding window cross-validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define the parameter grid for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Sample 50 random combinations of hyperparameters\n",
    "param_combinations = list(ParameterSampler(param_grid, n_iter=50, random_state=42))\n",
    "\n",
    "# Function to perform cross-validation with TimeSeriesSplit\n",
    "def tune_decision_tree(X, y, param_combinations, tscv):\n",
    "    best_params = None\n",
    "    lowest_mse = float(\"inf\")\n",
    "\n",
    "    # Iterate through all sampled parameter combinations\n",
    "    for params in param_combinations:\n",
    "        mse_scores = []\n",
    "\n",
    "        # Expanding window cross-validation\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_cv_train, X_cv_val = X[train_idx], X[val_idx]\n",
    "            y_cv_train, y_cv_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            # Initialize and fit Decision Tree with current parameters\n",
    "            model = DecisionTreeRegressor(**params, random_state=42)\n",
    "            model.fit(X_cv_train, y_cv_train)\n",
    "            y_pred = model.predict(X_cv_val)\n",
    "\n",
    "            # Calculate MSE for the current fold\n",
    "            mse = mean_squared_error(y_cv_val, y_pred)\n",
    "            mse_scores.append(mse)\n",
    "\n",
    "        # Calculate average MSE across splits\n",
    "        avg_mse = np.mean(mse_scores)\n",
    "\n",
    "        # Update the best parameters if the current average MSE is lower\n",
    "        if avg_mse < lowest_mse:\n",
    "            lowest_mse = avg_mse\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, lowest_mse\n",
    "\n",
    "# Perform hyperparameter tuning on the complete feature set\n",
    "X_all = pd.DataFrame(X_train_scaled, columns=X_train.columns).values\n",
    "y = y_train.values\n",
    "best_decision_tree_params, lowest_mse_all_features = tune_decision_tree(X_all, y, param_combinations, tscv)\n",
    "print(f\"Best Decision Tree hyperparameters for all features: {best_decision_tree_params}\")\n",
    "print(f\"Lowest MSE on all features: {lowest_mse_all_features:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZueDNZIT9TO"
   },
   "source": [
    "# **Train and Test Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZHF-PRCZOTR"
   },
   "source": [
    "**Defining Features from each Feature Selction Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rYRpIpT8Zkyg"
   },
   "outputs": [],
   "source": [
    "#Defining top features for each feature selection\n",
    "\n",
    "mfnn_top_features = ['News - Negative Sentiment_lag4', 'News - Stocks_lag2',\n",
    "       'News - New Products_lag1', 'News - Personnel Changes_lag2',\n",
    "       'News - Negative Sentiment_lag3', 'News - Positive Sentiment_lag2',\n",
    "       'News - Negative Sentiment_lag2', 'News - Adverse Events_lag2',\n",
    "       'News - Positive Sentiment_lag1', 'News - New Products_lag3',\n",
    "       'News - Store Openings_lag1', 'News - Positive Sentiment_lag3',\n",
    "       'Volume_lag4', 'News - Positive Sentiment_lag4',\n",
    "       'News - Corporate Earnings_lag1', 'Volume_lag3', 'Close_lag3',\n",
    "       'Close_lag2', 'Close_lag4', 'Close_lag1']\n",
    "\n",
    "close_prices = ['Close_lag1',\n",
    "                'Close_lag2',\n",
    "                'Close_lag3',\n",
    "                'Close_lag4',\n",
    "                'Close_lag5']\n",
    "\n",
    "lasso_top_features = ['News - Positive Sentiment_lag3',\n",
    " 'Volume_lag3',\n",
    " 'News - Stock Rumors_lag2',\n",
    " 'News - Personnel Changes_lag2',\n",
    " 'News - Adverse Events_lag2',\n",
    " 'News - Negative Sentiment_lag2',\n",
    " 'News - Negative Sentiment_lag3',\n",
    " 'News - Product Recalls_lag2',\n",
    " 'News - Mergers & Acquisitions_lag2',\n",
    " 'News - Corporate Earnings_lag2',\n",
    " 'News - Dividends_lag2',\n",
    " 'News - Stocks_lag2',\n",
    " 'News - Analyst Comments_lag2',\n",
    " 'News - Layoffs_lag2',\n",
    " 'News - Store Openings_lag2',\n",
    " 'News - New Products_lag3',\n",
    " 'Close_lag3',\n",
    " 'Close_lag5',\n",
    " 'Close_lag4',\n",
    " 'Close_lag1']\n",
    "\n",
    "svr_top_features = ['News - Negative Sentiment_lag2',\n",
    " 'News - Positive Sentiment_lag1',\n",
    " 'News - Adverse Events_lag3',\n",
    " 'Volume_lag3',\n",
    " 'News - Product Recalls_lag4',\n",
    " 'News - Negative Sentiment_lag5',\n",
    " 'News - Corporate Earnings_lag1',\n",
    " 'Volume_lag4',\n",
    " 'News - Product Recalls_lag1',\n",
    " 'News - Analyst Comments_lag2',\n",
    " 'News - Product Recalls_lag5',\n",
    " 'News - Analyst Comments_lag4',\n",
    " 'News - Analyst Comments_lag3',\n",
    " 'News - Stocks_lag5',\n",
    " 'News - Analyst Comments_lag5',\n",
    " 'Close_lag4',\n",
    " 'Close_lag5',\n",
    " 'Close_lag3',\n",
    " 'Close_lag2',\n",
    " 'Close_lag1']\n",
    "\n",
    "\n",
    "rf_top_features = ['News - Mergers & Acquisitions_lag5',\n",
    " 'News - Analyst Comments_lag4',\n",
    " 'News - Stocks_lag5',\n",
    " 'News - Analyst Comments_lag2',\n",
    " 'News - Stocks_lag4',\n",
    " 'News - Analyst Comments_lag3',\n",
    " 'News - Stocks_lag2',\n",
    " 'News - Analyst Comments_lag5',\n",
    " 'News - Mergers & Acquisitions_lag1',\n",
    " 'News - Corporate Earnings_lag1',\n",
    " 'Volume_lag4',\n",
    " 'Volume_lag3',\n",
    " 'Volume_lag1',\n",
    " 'Volume_lag5',\n",
    " 'Volume_lag2',\n",
    " 'Close_lag5',\n",
    " 'Close_lag4',\n",
    " 'Close_lag3',\n",
    " 'Close_lag2',\n",
    " 'Close_lag1']\n",
    "\n",
    "\n",
    "mffs_top_features = ['Volume_lag3',\n",
    " 'News - Positive Sentiment_lag3',\n",
    " 'News - Stock Rumors_lag2',\n",
    " 'News - Dividends_lag2',\n",
    " 'News - Layoffs_lag2',\n",
    " 'Close_lag3',\n",
    " 'News - Corporate Earnings_lag2',\n",
    " 'News - Product Recalls_lag2',\n",
    " 'News - Product Recalls_lag4',\n",
    " 'News - Analyst Comments_lag4',\n",
    " 'News - Stocks_lag5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yuUf_rnY7N5"
   },
   "source": [
    "**Defining Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSSSI6AKbB2y",
    "outputId": "1f05a357-4391-44ab-b06b-8533e858af63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Salman\\AppData\\Local\\Temp\\ipykernel_17828\\2178839820.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(\"train_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_data.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_data.columns)\n",
    "\n",
    "# Subset columns for training data based on selected features\n",
    "X_whole_train_all = X_train_scaled_df.copy()\n",
    "X_mfnn_train_all = X_train_scaled_df.loc[:, mfnn_top_features]\n",
    "X_lasso_train_all = X_train_scaled_df.loc[:, lasso_top_features]\n",
    "X_svr_train_all = X_train_scaled_df.loc[:, svr_top_features]\n",
    "X_rf_train_all = X_train_scaled_df.loc[:, rf_top_features]\n",
    "X_mffs_train_all = X_train_scaled_df.loc[:, mffs_top_features]\n",
    "X_close_train_all = X_train_scaled_df.loc[:, close_prices]\n",
    "\n",
    "#create X__test_all\n",
    "X_whole_test_all = X_test_scaled_df.copy()\n",
    "X_mfnn_test_all = X_test_scaled_df.loc[:, mfnn_top_features]\n",
    "X_lasso_test_all = X_test_scaled_df.loc[:, lasso_top_features]\n",
    "X_svr_test_all = X_test_scaled_df.loc[:, svr_top_features]\n",
    "X_rf_test_all = X_test_scaled_df.loc[:, rf_top_features]\n",
    "X_mffs_test_all = X_test_scaled_df.loc[:, mffs_top_features]\n",
    "X_close_test_all = X_test_scaled_df.loc[:, close_prices]\n",
    "\n",
    "# Create y\n",
    "y_train_all = y_train_data.copy()\n",
    "\n",
    "# Get indices of train_data that match the selected symbols\n",
    "selected_indices_aapl = train_data[train_data['Symbol'] == 'AAPL'].index\n",
    "\n",
    "X_whole_test_aapl = X_whole_test_all[test_data['Symbol'] == 'AAPL']\n",
    "X_mfnn_test_aapl = X_mfnn_test_all[test_data['Symbol'] == 'AAPL']\n",
    "X_lasso_test_aapl = X_lasso_test_all[test_data['Symbol'] == 'AAPL']\n",
    "X_svr_test_aapl = X_svr_test_all[test_data['Symbol'] == 'AAPL']\n",
    "X_rf_test_aapl = X_rf_test_all[test_data['Symbol'] == 'AAPL']\n",
    "X_mffs_test_aapl = X_mffs_test_all[test_data['Symbol'] == 'AAPL']\n",
    "X_close_test_aapl = X_close_test_all[test_data['Symbol'] == 'AAPL']\n",
    "\n",
    "# Get indices of test_data that match the selected symbols\n",
    "selected_indices_aapl = test_data[test_data['Symbol'] == 'AAPL'].index\n",
    "\n",
    "# Filter y_test based on the selected indices\n",
    "y_test_aapl = y_test_data.loc[selected_indices_aapl]\n",
    "\n",
    "# Define datasets dictionary\n",
    "datasets_all = {\n",
    "    'Whole_all': (X_whole_train_all, X_whole_test_aapl),\n",
    "    'MFNN_Top_all': (X_mfnn_train_all, X_mfnn_test_aapl),\n",
    "    'Lasso_Top_all': (X_lasso_train_all, X_lasso_test_aapl),\n",
    "    'SVR_Top_all': (X_svr_train_all, X_svr_test_aapl),\n",
    "    'RF_Top_all': (X_rf_train_all, X_rf_test_aapl),\n",
    "    'MFFS_Top_all': (X_mffs_train_all, X_mffs_test_aapl),\n",
    "    'Close_Prices_all': (X_close_train_all, X_close_test_aapl)\n",
    "}\n",
    "\n",
    "# Creating train_data for some companies\n",
    "\n",
    "# Get unique company symbols\n",
    "symbols = train_data['Symbol'].unique()\n",
    "\n",
    "# Select half of the remaining symbols\n",
    "half_symbols = symbols[:len(symbols) // 2]\n",
    "\n",
    "X_whole_train_some = X_whole_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "X_mfnn_train_some = X_mfnn_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "X_lasso_train_some = X_lasso_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "X_svr_train_some = X_svr_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "X_rf_train_some = X_rf_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "X_mffs_train_some = X_mffs_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "X_close_train_some = X_close_train_all[~train_data['Symbol'].isin(half_symbols)]\n",
    "\n",
    "# Get indices of train_data that match the selected symbols\n",
    "selected_indices = train_data[~train_data['Symbol'].isin(half_symbols)].index\n",
    "\n",
    "# Filter y_train based on the selected indices\n",
    "y_train_some = y_train_data.loc[selected_indices]\n",
    "\n",
    "datasets_some = {\n",
    "    'Whole_some': (X_whole_train_some, X_whole_test_aapl),\n",
    "    'MFNN_Top_some': (X_mfnn_train_some, X_mfnn_test_aapl),\n",
    "    'Lasso_Top_some': (X_lasso_train_some, X_lasso_test_aapl),\n",
    "    'SVR_Top_some': (X_svr_train_some, X_svr_test_aapl),\n",
    "    'RF_Top_some': (X_rf_train_some, X_rf_test_aapl),\n",
    "    'MFFS_Top_some': (X_mffs_train_some, X_mffs_test_aapl),\n",
    "    'Close_Prices_some': (X_close_train_some, X_close_test_aapl)\n",
    "}\n",
    "\n",
    "# AAPL\n",
    "X_whole_train_aapl = X_whole_train_all[train_data['Symbol'] == 'AAPL']\n",
    "X_mfnn_train_aapl = X_mfnn_train_all[train_data['Symbol'] == 'AAPL']\n",
    "X_lasso_train_aapl = X_lasso_train_all[train_data['Symbol'] == 'AAPL']\n",
    "X_svr_train_aapl = X_svr_train_all[train_data['Symbol'] == 'AAPL']\n",
    "X_rf_train_aapl = X_rf_train_all[train_data['Symbol'] == 'AAPL']\n",
    "X_mffs_train_aapl = X_mffs_train_all[train_data['Symbol'] == 'AAPL']\n",
    "X_close_train_aapl = X_close_train_all[train_data['Symbol'] == 'AAPL']\n",
    "\n",
    "# Get indices of train_data that match the selected symbols\n",
    "selected_indices_aapl = train_data[train_data['Symbol'] == 'AAPL'].index\n",
    "\n",
    "# Filter y_train based on the selected indices\n",
    "y_train_aapl = y_train_data.loc[selected_indices_aapl]\n",
    "\n",
    "datasets_aapl = {\n",
    "    'Whole_aapl': (X_whole_train_aapl, X_whole_test_aapl),\n",
    "    'MFNN_Top_aapl': (X_mfnn_train_aapl, X_mfnn_test_aapl),\n",
    "    'Lasso_Top_aapl': (X_lasso_train_aapl, X_lasso_test_aapl),\n",
    "    'SVR_Top_aapl': (X_svr_train_aapl, X_svr_test_aapl),\n",
    "    'RF_Top_aapl': (X_rf_train_aapl, X_rf_test_aapl),\n",
    "    'MFFS_Top_aapl': (X_mffs_train_aapl, X_mffs_test_aapl),\n",
    "    'Close_Prices_aapl': (X_close_train_aapl, X_close_test_aapl)\n",
    "}\n",
    "\n",
    "# Define filters for each company in the train and test datasets\n",
    "aapl_train_filter = train_data['Symbol'] == 'AAPL'\n",
    "aapl_test_filter = test_data['Symbol'] == 'AAPL'\n",
    "\n",
    "# Select only the closing price columns for each company in train and test datasets\n",
    "unscaled_X_train_close_aapl = X_train_data.loc[aapl_train_filter, close_prices]\n",
    "unscaled_X_test_close_aapl = X_test_data.loc[aapl_test_filter, close_prices]\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
    "\n",
    "# Prepare data for ARIMA by setting date to index and filtering only for each company\n",
    "\n",
    "# For AAPL\n",
    "train_data_aapl = train_data[train_data['Symbol'] == 'AAPL'].copy()\n",
    "train_data_aapl.set_index('Date', inplace=True)\n",
    "\n",
    "test_data_aapl = test_data[test_data['Symbol'] == 'AAPL'].copy()\n",
    "test_data_aapl.set_index('Date', inplace=True)\n",
    "\n",
    "# AAPL Close price data for ARIMA\n",
    "close_only_train_aapl = train_data_aapl['Close']\n",
    "close_only_test_aapl = test_data_aapl['Close']\n",
    "\n",
    "# Define all datasets and their respective y_train and y_test\n",
    "datasets_dict = {\n",
    "    'All_Company': (datasets_all, y_train_all, y_test_aapl),\n",
    "    'Some_Company': (datasets_some, y_train_some, y_test_aapl),\n",
    "    'AAPL': (datasets_aapl, y_train_aapl, y_test_aapl),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Results\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtghBDLLcLKP"
   },
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syrUS-23YwrT"
   },
   "outputs": [],
   "source": [
    "# Initialize the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Function to evaluate model, save results, and append to `results`\n",
    "def evaluate_and_save_results_linear(dataset_name, X_train, X_test, y_train, y_test, method_label, results_folder):\n",
    "    # Create the folder if it does not exist\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    \n",
    "    # Fit the model and make predictions\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    y_pred_train_values = linear_model.predict(X_train)\n",
    "    y_pred_test_values = linear_model.predict(X_test)\n",
    "    \n",
    "    # Create DataFrames for train and test predictions with unique names\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train_values, index=y_train.index, columns=[f'y_pred_{dataset_name}_train'])\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test_values, index=y_test.index, columns=[f'y_pred_{dataset_name}_test'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train_values)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train, y_pred_train_values))\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train_values)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test_values)\n",
    "    rmse_test = sqrt(mean_squared_error(y_test, y_pred_test_values))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test_values)\n",
    "    \n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': 'Linear Regression',\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle with a filename that includes the dataset and method\n",
    "    model_filename = os.path.join(results_folder, f'linear_model_{method_label}_{dataset_name}.pkl')\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(linear_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred_df, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train_df, y_pred_test_df],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred_df.iloc[:, 0], label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'Linear Regression - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Data point')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plot_filename = os.path.join(results_folder, f'actual_vs_predicted_{data_type}_Linear_Regression_{dataset_name}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "# Define the folder to save all results\n",
    "results_folder = 'linear_model_results'\n",
    "\n",
    "# Loop through each dataset type and evaluate, storing specified predictions in dictionary\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        evaluate_and_save_results_linear(dataset_name, X_train, X_test, y_train, y_test, method_label, results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEX3InpIfna1"
   },
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH4NoPFQfmyR"
   },
   "outputs": [],
   "source": [
    "# Define the folder to save all results\n",
    "results_folder = 'lasso_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# Define the range of alphas to search over\n",
    "alphas = np.logspace(-3, 1, 50)  # Adjust range as needed\n",
    "\n",
    "# Initialize LassoCV with TimeSeriesSplit\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=tscv, max_iter=10000)\n",
    "\n",
    "# Fit LassoCV on the entire training data to find the best alpha\n",
    "lasso_cv.fit(X_train_scaled_df, y_train_data)  # Use the whole dataset for initial tuning\n",
    "\n",
    "# Get the best alpha value\n",
    "best_alpha = lasso_cv.alpha_\n",
    "print(f\"Optimal alpha: {best_alpha}\")\n",
    "\n",
    "# Assuming best_alpha is found from LassoCV with expanding window cross-validation\n",
    "lasso_model = Lasso(alpha=best_alpha)\n",
    "\n",
    "# Function to evaluate model, save results, and append to `results`\n",
    "def evaluate_and_save_results_lasso(dataset_name, X_train, X_test, y_train, y_test, method_label):\n",
    "    # Fit the model and make predictions\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    y_pred_train_values = lasso_model.predict(X_train)\n",
    "    y_pred_test_values = lasso_model.predict(X_test)\n",
    "    \n",
    "    # Create DataFrames for train and test predictions with unique names\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train_values, index=y_train.index, columns=[f'y_pred_{dataset_name}_train'])\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test_values, index=y_test.index, columns=[f'y_pred_{dataset_name}_test'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train_values)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train, y_pred_train_values))\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train_values)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test_values)\n",
    "    rmse_test = sqrt(mean_squared_error(y_test, y_pred_test_values))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test_values)\n",
    "    \n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': 'Lasso Regression',\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle with a filename that includes the dataset and method\n",
    "    model_filename = os.path.join(results_folder, f'lasso_model_{method_label}_{dataset_name}.pkl')\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(lasso_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred_df, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train_df, y_pred_test_df],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred_df.iloc[:, 0], label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'Lasso Regression - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Data point')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plot_filename = os.path.join(results_folder, f'actual_vs_predicted_{data_type}_Lasso_Regression_{dataset_name}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate, storing specified predictions in dictionary\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        evaluate_and_save_results_lasso(dataset_name, X_train, X_test, y_train, y_test, method_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Walk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the folder to save all results\n",
    "results_folder = 'random_walk_with_noise_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# Loop through each company dataset in datasets_dict\n",
    "for company, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    # Apply only to the specific company 'AAPL'\n",
    "    if company == 'AAPL':\n",
    "        # Training set: Predict using previous value with added Gaussian noise for the next day (except for the first value)\n",
    "        noise_train = np.random.normal(0, y_train.std() * 0.01, len(y_train))  # Gaussian noise with mean 0 and small std deviation\n",
    "        y_pred_rw_train = y_train.shift(1).fillna(method='bfill') + noise_train  # Add noise to the shifted values\n",
    "\n",
    "        # Testing set: Predict using the previous day's actual value, adding Gaussian noise for each day\n",
    "        y_pred_rw_test_list = []\n",
    "\n",
    "        # Iterate over each day in the test set to generate predictions using true values from previous day\n",
    "        for i in range(len(y_test)):\n",
    "            previous_value = y_train.iloc[-1] if i == 0 else y_test.iloc[i - 1]  # Use last training value for the first day, then true value for subsequent\n",
    "            noise_test = np.random.normal(0, y_train.std() * 0.01)  # Gaussian noise\n",
    "            predicted_value = previous_value + noise_test\n",
    "            y_pred_rw_test_list.append(predicted_value)\n",
    "\n",
    "        # Convert the predictions to a Pandas Series\n",
    "        y_pred_rw_test_series = pd.Series(y_pred_rw_test_list, index=y_test.index)\n",
    "\n",
    "        # Evaluate metrics for both training and testing data\n",
    "        r2_train = r2_score(y_train, y_pred_rw_train)\n",
    "        rmse_train = mean_squared_error(y_train, y_pred_rw_train, squared=False)\n",
    "        mae_train = mean_absolute_error(y_train, y_pred_rw_train)\n",
    "\n",
    "        r2_test = r2_score(y_test, y_pred_rw_test_series)\n",
    "        rmse_test = mean_squared_error(y_test, y_pred_rw_test_series, squared=False)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_rw_test_series)\n",
    "\n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            'Dataset': 'Close Price',\n",
    "            'Method': f'{company} only',\n",
    "            'Model': 'Random Walk with Noise',\n",
    "            'R2_Train': r2_train,\n",
    "            'RMSE_Train': rmse_train,\n",
    "            'MAE_Train': mae_train,\n",
    "            'R2_Test': r2_test,\n",
    "            'RMSE_Test': rmse_test,\n",
    "            'MAE_Test': mae_test\n",
    "        })\n",
    "\n",
    "        # Plot actual vs predicted closing prices for training data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_train.index, y_train, label='Actual Train', color='blue')\n",
    "        plt.plot(y_train.index, y_pred_rw_train, label='Predicted Train', color='red')\n",
    "        plt.title(f'Random Walk with Noise - Actual vs Predicted Closing Price (Training) - {company}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(results_folder, f'actual_vs_predicted_train_{company}_RW_Noise_True_Close_Price.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot actual vs predicted closing prices for testing data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_test.index, y_test, label='Actual Test', color='blue')\n",
    "        plt.plot(y_test.index, y_pred_rw_test_series, label='Predicted Test', color='red')\n",
    "        plt.title(f'Random Walk with Noise - Actual vs Predicted Closing Price (Testing) - {company}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(results_folder, f'actual_vs_predicted_test_{company}_RW_Noise_True_Close_Price.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Moving Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save all results\n",
    "results_folder = 'sma_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# List of companies to process\n",
    "companies = ['AAPL']\n",
    "\n",
    "# Dictionary to access each company's train and test data\n",
    "unscaled_data = {\n",
    "    'AAPL': (unscaled_X_train_close_aapl, unscaled_X_test_close_aapl, y_train_aapl, y_test_aapl)\n",
    "}\n",
    "\n",
    "# Loop over each company and perform SMA calculations\n",
    "for company, (unscaled_X_train_close, unscaled_X_test_close, y_train, y_test) in unscaled_data.items():\n",
    "    \n",
    "    # Calculate SMA predictions for train and test data\n",
    "    y_pred_sma_train = unscaled_X_train_close.mean(axis=1)\n",
    "    y_pred_sma_test = unscaled_X_test_close.mean(axis=1)\n",
    "\n",
    "    # Save predictions in DataFrames with appropriate names\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_sma_train, index=y_train.index, columns=[f'y_pred_{company}_train'])\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_sma_test, index=y_test.index, columns=[f'y_pred_{company}_test'])\n",
    "\n",
    "    # Evaluate metrics for both training and testing data\n",
    "    r2_train = r2_score(y_train, y_pred_sma_train)\n",
    "    rmse_train = mean_squared_error(y_train, y_pred_sma_train, squared=False)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_sma_train)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_sma_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred_sma_test, squared=False)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_sma_test)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        'Dataset': 'Close Price',\n",
    "        'Method': f'{company} only',\n",
    "        'Model': 'SMA',\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Plot actual vs predicted closing prices for training data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_train.index, y_train, label='Actual Train', color='blue')\n",
    "    plt.plot(y_train.index, y_pred_sma_train, label='Predicted Train', color='red')\n",
    "    plt.title(f'SMA - Actual vs Predicted Closing Price (Training) - {company} Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_folder, f'actual_vs_predicted_train_{company}_SMA_Close_Price.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot actual vs predicted closing prices for testing data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test.index, y_test, label='Actual Test', color='blue')\n",
    "    plt.plot(y_test.index, y_pred_sma_test, label='Predicted Test', color='red')\n",
    "    plt.title(f'SMA - Actual vs Predicted Closing Price (Testing) - {company} Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_folder, f'actual_vs_predicted_test_{company}_SMA_Close_Price.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARIMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save all results\n",
    "results_folder = 'arima_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# Dictionary containing the training and test data for each firm\n",
    "firms_data_arima = {\n",
    "    'AAPL': (close_only_train_aapl, close_only_test_aapl)\n",
    "}\n",
    "\n",
    "# Loop over each firm\n",
    "for firm, (close_only_train, close_only_test) in firms_data_arima.items():\n",
    "    # Step 1: Find the best ARIMA parameters using auto_arima\n",
    "    model = auto_arima(\n",
    "        close_only_train,\n",
    "        start_p=0, max_p=5,\n",
    "        start_q=0, max_q=5,\n",
    "        d=None,\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True\n",
    "    )\n",
    "    p, d, q = model.order\n",
    "\n",
    "    # Step 2: Fit a SARIMAX model with the best parameters on the initial training data\n",
    "    arimamodel = SARIMAX(close_only_train, order=(p, d, q))\n",
    "    arimamodel_fit = arimamodel.fit(disp=False)\n",
    "\n",
    "    # Step 3: Initialize list to store rolling predictions\n",
    "    rolling_forecast = []\n",
    "    observed_values = close_only_train.copy()\n",
    "\n",
    "    # Step 4: Generate rolling predictions without re-fitting the model\n",
    "    for i, actual_value in enumerate(close_only_test):\n",
    "        # Predict the next time step\n",
    "        forecast = arimamodel_fit.get_forecast(steps=1).predicted_mean.iloc[0]\n",
    "        rolling_forecast.append(forecast)\n",
    "        \n",
    "        # Add the actual observed value to observed_values\n",
    "        new_observed = pd.Series([actual_value], index=[close_only_test.index[i]])\n",
    "        observed_values = pd.concat([observed_values, new_observed])\n",
    "\n",
    "        # Reinitialize the SARIMAX model with updated observed data, keeping fitted parameters\n",
    "        arimamodel = SARIMAX(observed_values, order=(p, d, q))\n",
    "        arimamodel_fit = arimamodel.filter(arimamodel_fit.params)\n",
    "\n",
    "    # Convert rolling_forecast to a pandas Series for easy comparison\n",
    "    rolling_forecast_series = pd.Series(rolling_forecast, index=close_only_test.index)\n",
    "\n",
    "    # Step 5: Calculate R2, RMSE, and MAE for the test set\n",
    "    r2_test = r2_score(close_only_test, rolling_forecast_series)\n",
    "    rmse_test = mean_squared_error(close_only_test, rolling_forecast_series, squared=False)\n",
    "    mae_test = mean_absolute_error(close_only_test, rolling_forecast_series)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        'Dataset': f'{firm} Close Price',\n",
    "        'Method': f'{firm} Only',\n",
    "        'Model': 'ARIMA',\n",
    "        'R2_Train': '-',  # No training R2 provided\n",
    "        'RMSE_Train': '-',  # No training RMSE provided\n",
    "        'MAE_Train': '-',  # No training MAE provided\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model with pickle\n",
    "    model_filename = os.path.join(results_folder, f'arima_model_{firm}_only.pkl')\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(arimamodel_fit, model_file)\n",
    "\n",
    "    # Plot actual vs rolling forecasted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(close_only_train.index, close_only_train, label='Training Data')\n",
    "    plt.plot(close_only_test.index, close_only_test, label='Actual Test Data')\n",
    "    plt.plot(close_only_test.index, rolling_forecast_series, label='Rolling Forecasted Data', linestyle='--')\n",
    "    plt.xlabel('Data Point')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.title(f'{firm} Actual vs Rolling Forecast ARIMA Predictions')\n",
    "    plt.legend()\n",
    "    plot_filename = os.path.join(results_folder, f'actual_vs_rolling_forecast_{firm}_ARIMA.png')\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save all results\n",
    "results_folder = 'xgboost_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# Dictionary to store XGBoost model configurations based on hyperparameter tuning methods\n",
    "xgboost_configs = {\n",
    "    'XGBoost_RandomizedSearch': {\n",
    "        'subsample': 1.0,\n",
    "        'reg_lambda': 1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'n_estimators': 150,\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.1,\n",
    "        'gamma': 0,\n",
    "        'colsample_bytree': 0.8\n",
    "    },\n",
    "    'XGBoost_Optuna': {\n",
    "        'max_depth': 3,\n",
    "        'min_child_weight': 2,\n",
    "        'gamma': 0.4,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 0.4,\n",
    "        'learning_rate': 0.17217120489054266,\n",
    "        'n_estimators': 200,\n",
    "        'alpha': 0.015033446295092711,\n",
    "        'lambda': 2.4439050147224077\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to evaluate model, save results, and append to `results`\n",
    "def evaluate_and_save_results_xgboost(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, xgb_params):\n",
    "    # Initialize the XGBoost model with specified parameters\n",
    "    xgb_model = XGBRegressor(**xgb_params)\n",
    "    \n",
    "    # Fit the model and make predictions\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_train_values = xgb_model.predict(X_train)\n",
    "    y_pred_test_values = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train_values)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train, y_pred_train_values))\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train_values)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test_values)\n",
    "    rmse_test = sqrt(mean_squared_error(y_test, y_pred_test_values))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test_values)\n",
    "    \n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': config_name,\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle with a filename that includes the dataset, method, and configuration name\n",
    "    model_filename = os.path.join(results_folder, f'xgboost_model_{config_name}_{method_label}_{dataset_name}.pkl')\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(xgb_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred_values, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train_values, y_pred_test_values],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred_values, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'{config_name} - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Data point')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plot_filename = os.path.join(results_folder, f'actual_vs_predicted_{data_type}_{config_name}_{dataset_name}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate for each XGBoost configuration\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        for config_name, xgb_params in xgboost_configs.items():\n",
    "            evaluate_and_save_results_xgboost(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, xgb_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save all results\n",
    "results_folder = 'catboost_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# Dictionary to store CatBoost model configurations based on hyperparameter tuning methods\n",
    "catboost_configs = {\n",
    "    'CatBoost_RandomizedSearch': {\n",
    "        'subsample': 1.0,\n",
    "        'learning_rate': 0.1,\n",
    "        'l2_leaf_reg': 5,\n",
    "        'iterations': 50,\n",
    "        'depth': 5,\n",
    "        'border_count': 128\n",
    "    },\n",
    "    'CatBoost_Optuna': {\n",
    "        'subsample': 0.7199681031900936,\n",
    "        'random_strength': 8,\n",
    "        'random_seed': 55,\n",
    "        'learning_rate': 0.02855751764303586,\n",
    "        'l2_leaf_reg': 1.0878665873159017,\n",
    "        'iterations': 1000,\n",
    "        'depth': 3,\n",
    "        'colsample_bylevel': 0.9462619361669599,\n",
    "        'border_count': 203,\n",
    "        'bagging_temperature': 0.8379990713830481\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to evaluate model, save results, and append to `results`\n",
    "def evaluate_and_save_results_catboost(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, cb_params):\n",
    "    # Initialize the CatBoost model with specified parameters\n",
    "    cb_model = CatBoostRegressor(**cb_params, verbose=0)  # Suppress output with verbose=0\n",
    "    \n",
    "    # Fit the model and make predictions\n",
    "    cb_model.fit(X_train, y_train)\n",
    "    y_pred_train_values = cb_model.predict(X_train)\n",
    "    y_pred_test_values = cb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train_values)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train, y_pred_train_values))\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train_values)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test_values)\n",
    "    rmse_test = sqrt(mean_squared_error(y_test, y_pred_test_values))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test_values)\n",
    "    \n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': config_name,\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle with a filename that includes the dataset, method, and configuration name\n",
    "    model_filename = os.path.join(results_folder, f'catboost_model_{config_name}_{method_label}_{dataset_name}.pkl')\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(cb_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred_values, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train_values, y_pred_test_values],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred_values, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'{config_name} - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Data point')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plot_filename = os.path.join(results_folder, f'actual_vs_predicted_{data_type}_{config_name}_{dataset_name}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate for each CatBoost configuration\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        for config_name, cb_params in catboost_configs.items():\n",
    "            evaluate_and_save_results_catboost(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save all results\n",
    "results_folder = 'random_forest_results'\n",
    "os.makedirs(results_folder, exist_ok=True)  # Create the folder if it does not exist\n",
    "\n",
    "# Dictionary to store Random Forest model configurations based on hyperparameter tuning methods\n",
    "rf_configs = {\n",
    "    'RandomForest_RandomizedSearch': {\n",
    "        'n_estimators': 150,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': None,\n",
    "        'max_depth': 15,\n",
    "        'bootstrap': False\n",
    "    },\n",
    "    'RandomForest_Optuna': {\n",
    "        'n_estimators': 100,\n",
    "        'min_samples_split': 4,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': None,\n",
    "        'max_depth': 11,\n",
    "        'bootstrap': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to evaluate model, save results, and append to `results`\n",
    "def evaluate_and_save_results_rf(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, rf_params):\n",
    "    # Initialize the Random Forest model with specified parameters\n",
    "    rf_model = RandomForestRegressor(**rf_params)\n",
    "    \n",
    "    # Fit the model and make predictions\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_train_values = rf_model.predict(X_train)\n",
    "    y_pred_test_values = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train_values)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train, y_pred_train_values))\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train_values)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test_values)\n",
    "    rmse_test = sqrt(mean_squared_error(y_test, y_pred_test_values))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test_values)\n",
    "    \n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': config_name,\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle with a filename that includes the dataset, method, and configuration name\n",
    "    model_filename = os.path.join(results_folder, f'rf_model_{config_name}_{method_label}_{dataset_name}.pkl')\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(rf_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred_values, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train_values, y_pred_test_values],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred_values, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'{config_name} - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Data point')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plot_filename = os.path.join(results_folder, f'actual_vs_predicted_{data_type}_{config_name}_{dataset_name}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate for each Random Forest configuration\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        for config_name, rf_params in rf_configs.items():\n",
    "            evaluate_and_save_results_rf(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values(by=\"RMSE_Test\", ascending=True)\n",
    "print(results_df_sorted)\n",
    "results_df_sorted.to_csv('sorted_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANN using Random Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Updated ANN parameters dictionary for RGS\n",
    "ANN_params_rgs = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'layers': 5,\n",
    "    'neurons': 16,\n",
    "    'activation': 'relu',\n",
    "    'learning_rate': 0.01,\n",
    "    'dropout_rate': 0.5,\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "# Function to build and compile the ANN model for RGS\n",
    "def build_ANN_model_rgs(input_shape):\n",
    "    model = Sequential()\n",
    "    for _ in range(ANN_params_rgs['layers']):\n",
    "        model.add(Dense(ANN_params_rgs['neurons'], activation=ANN_params_rgs['activation'], input_shape=(input_shape,)))\n",
    "        model.add(Dropout(ANN_params_rgs['dropout_rate']))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "    optimizer = Adam(learning_rate=ANN_params_rgs['learning_rate']) if ANN_params_rgs['optimizer'] == 'adam' else SGD(learning_rate=ANN_params_rgs['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate ANN for each dataset using RGS hyperparameters\n",
    "def evaluate_and_save_results_ANN_rgs(dataset_name, X_train, X_test, y_train, y_test, method_label):\n",
    "    # Build and train the ANN model\n",
    "    ann_model = build_ANN_model_rgs(X_train.shape[1])\n",
    "    ann_model.fit(X_train, y_train, epochs=ANN_params_rgs['epochs'], batch_size=ANN_params_rgs['batch_size'], verbose=0)\n",
    "\n",
    "    # Predict on train and test data\n",
    "    y_pred_train = ann_model.predict(X_train).flatten()\n",
    "    y_pred_test = ann_model.predict(X_test).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': 'ANN Regression (RGS)',\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle\n",
    "    model_filename = f'ann_model_rgs_{method_label}_{dataset_name}.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(ann_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train, y_pred_test],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'ANN Regression (RGS) - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'actual_vs_predicted_{data_type}_ANN_Regression_RGS_{dataset_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate using RGS hyperparameters\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        evaluate_and_save_results_ANN_rgs(dataset_name, X_train, X_test, y_train, y_test, method_label)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('ANN_Regression_RandomGridSearch_Results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANN Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Updated ANN parameters dictionary for Optuna\n",
    "ANN_params_optuna = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 44,\n",
    "    'layers': 3,\n",
    "    'neurons': 64,\n",
    "    'activation': 'relu',\n",
    "    'learning_rate': 0.00022424536912470238,\n",
    "    'dropout_rate': 0.1562540817150868,\n",
    "    'optimizer': 'adam'\n",
    "}\n",
    "\n",
    "# Function to build and compile the ANN model for Optuna\n",
    "def build_ANN_model_optuna(input_shape):\n",
    "    model = Sequential()\n",
    "    for _ in range(ANN_params_optuna['layers']):\n",
    "        model.add(Dense(ANN_params_optuna['neurons'], activation=ANN_params_optuna['activation'], input_shape=(input_shape,)))\n",
    "        model.add(Dropout(ANN_params_optuna['dropout_rate']))\n",
    "    model.add(Dense(1))  \n",
    "\n",
    "    optimizer = Adam(learning_rate=ANN_params_optuna['learning_rate']) if ANN_params_optuna['optimizer'] == 'adam' else SGD(learning_rate=ANN_params_optuna['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate ANN for each dataset using Optuna hyperparameters\n",
    "def evaluate_and_save_results_ANN_optuna(dataset_name, X_train, X_test, y_train, y_test, method_label):\n",
    "    # Build and train the ANN model\n",
    "    ann_model = build_ANN_model_optuna(X_train.shape[1])\n",
    "    ann_model.fit(X_train, y_train, epochs=ANN_params_optuna['epochs'], batch_size=ANN_params_optuna['batch_size'], verbose=0)\n",
    "\n",
    "    # Predict on train and test data\n",
    "    y_pred_train = ann_model.predict(X_train).flatten()\n",
    "    y_pred_test = ann_model.predict(X_test).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "    # Append results to the already defined `results` list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': 'ANN Regression (Optuna)',\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle\n",
    "    model_filename = f'ann_model_optuna_{method_label}_{dataset_name}.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(ann_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train, y_test],\n",
    "        [y_pred_train, y_pred_test],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true.index, y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(y_true.index, y_pred, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'ANN Regression (Optuna) - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'actual_vs_predicted_{data_type}_ANN_Regression_Optuna_{dataset_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate using Optuna hyperparameters\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        evaluate_and_save_results_ANN_optuna(dataset_name, X_train, X_test, y_train, y_test, method_label)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('ANN_Regression_Optuna_Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data for LSTM, RNN, GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full training and testing datasets\n",
    "train_data = pd.read_csv(\"train_data_lstm.csv\")\n",
    "test_data = pd.read_csv(\"test_data_lstm.csv\")\n",
    "\n",
    "# Define the target and features\n",
    "target_column = 'Close'\n",
    "feature_columns = [col for col in train_data.columns if col not in ['Date', 'Symbol', 'GICS Sector', 'Headquarters Location', 'Founded']]\n",
    "\n",
    "# Prepare sequences function with debugging to verify shapes\n",
    "def prepare_sequences(data, sequence_length=5):\n",
    "    X, y = [], []\n",
    "    grouped_data = data.groupby('Symbol')\n",
    "    \n",
    "    for symbol, group in grouped_data:\n",
    "        group = group.sort_values(by='Date')\n",
    "        features = group[feature_columns].values\n",
    "        target = group['Close'].values\n",
    "\n",
    "        for i in range(len(features) - sequence_length):\n",
    "            X.append(features[i:i + sequence_length])\n",
    "            y.append(target[i + sequence_length])\n",
    "\n",
    "    # Convert lists to numpy arrays and check shapes\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(f\"Prepared sequences: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "def create_scaled_sequences(train_data, test_data, symbols=None, sequence_length=5):\n",
    "    # Filter the data if symbols are provided\n",
    "    if symbols is not None:\n",
    "        train_data = train_data[train_data['Symbol'].isin(symbols)]\n",
    "        test_data = test_data[test_data['Symbol'].isin(symbols)]\n",
    "\n",
    "    # Prepare sequences\n",
    "    X_train_raw, y_train_raw = prepare_sequences(train_data, sequence_length)\n",
    "    X_test_raw, y_test_raw = prepare_sequences(test_data, sequence_length)\n",
    "\n",
    "    # Check the shapes before scaling\n",
    "    print(f\"Raw shapes before scaling: X_train shape = {X_train_raw.shape}, y_train shape = {y_train_raw.shape}\")\n",
    "    print(f\"Raw shapes before scaling: X_test shape = {X_test_raw.shape}, y_test shape = {y_test_raw.shape}\")\n",
    "\n",
    "    # Initialize scalers\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale features and target\n",
    "    X_train = feature_scaler.fit_transform(X_train_raw.reshape(-1, X_train_raw.shape[-1])).reshape(X_train_raw.shape)\n",
    "    X_test = feature_scaler.transform(X_test_raw.reshape(-1, X_test_raw.shape[-1])).reshape(X_test_raw.shape)\n",
    "    y_train = target_scaler.fit_transform(y_train_raw.reshape(-1, 1)).flatten()\n",
    "    y_test = target_scaler.transform(y_test_raw.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Check the shapes after scaling\n",
    "    print(f\"Scaled shapes: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
    "    print(f\"Scaled shapes: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, feature_scaler, target_scaler, X_test, y_test, feature_scaler, target_scaler\n",
    "\n",
    "# Now proceed with creating datasets_dict and running your model training and evaluation code as before.\n",
    "\n",
    "# Adjust calls to `create_scaled_sequences` to unpack into four items\n",
    "\n",
    "X_train_all, y_train_all, feature_scaler_all, target_scaler_all, X_test_all, y_test_all, feature_scaler_test_all, target_scaler_test_all = create_scaled_sequences(train_data, test_data)\n",
    "\n",
    "# Prepare datasets for some companies (first half of symbols)\n",
    "half_symbols = train_data['Symbol'].unique()[:len(train_data['Symbol'].unique()) // 2]\n",
    "X_train_some, y_train_some, feature_scaler_some, target_scaler_some, X_test_some, y_test_some, feature_scaler_test_some, target_scaler_test_some = create_scaled_sequences(train_data, test_data, symbols=half_symbols)\n",
    "\n",
    "# Populate `datasets_dict` with properly unpacked values\n",
    "datasets_dict = {\n",
    "    'All_Company': (X_train_all, y_train_all, feature_scaler_all, target_scaler_all),\n",
    "    'Some_Company': (X_train_some, y_train_some, feature_scaler_some, target_scaler_some)\n",
    "}\n",
    "\n",
    "for symbol in ['AAPL']:\n",
    "    X_train, y_train, feature_scaler, target_scaler, X_test, y_test, feature_scaler_test, target_scaler_test = create_scaled_sequences(train_data, test_data, symbols=[symbol])\n",
    "    datasets_dict[symbol] = (X_train, y_train, feature_scaler, target_scaler)\n",
    "    datasets_dict[f\"{symbol}_Test\"] = (X_test, y_test, feature_scaler_test, target_scaler_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define both sets of LSTM hyperparameters\n",
    "lstm_params_rgs = {\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.005,\n",
    "    'n_layers': 5,\n",
    "    'n_units': 100,\n",
    "    'dropout_rate': 0.2,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "lstm_params_optuna = {\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.006431528679768236,\n",
    "    'n_layers': 7,\n",
    "    'n_units': 54,\n",
    "    'dropout_rate': 0.24985937241800743,\n",
    "    'epochs': 15,\n",
    "    'batch_size': 128\n",
    "}\n",
    "\n",
    "# Define both sets of hyperparameters to iterate over\n",
    "parameter_sets = {\n",
    "    'RGS': lstm_params_rgs,\n",
    "    'Optuna': lstm_params_optuna\n",
    "}\n",
    "\n",
    "# Function to build the LSTM model\n",
    "def build_lstm_model(input_shape, params):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    for i in range(params['n_layers']):\n",
    "        return_sequences = i < params['n_layers'] - 1\n",
    "        model.add(LSTM(params['n_units'], return_sequences=return_sequences))\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Function to evaluate and save LSTM results for each train-test-parameter set combination\n",
    "def evaluate_and_save_results_lstm(train_name, test_name, X_train, X_test, y_train, y_test, target_scaler_train, target_scaler_test, param_label, params):\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    lstm_model = build_lstm_model(input_shape, params)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with the given parameters\n",
    "    lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size'],\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict and calculate metrics on train data\n",
    "    y_pred_train = target_scaler_train.inverse_transform(lstm_model.predict(X_train).flatten().reshape(-1, 1)).flatten()\n",
    "    y_train_original = target_scaler_train.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    r2_train = r2_score(y_train_original, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_original, y_pred_train))\n",
    "    mae_train = mean_absolute_error(y_train_original, y_pred_train)\n",
    "\n",
    "    # Predict and calculate metrics on test data\n",
    "    y_pred_test = target_scaler_test.inverse_transform(lstm_model.predict(X_test).flatten().reshape(-1, 1)).flatten()\n",
    "    y_test_original = target_scaler_test.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    r2_test = r2_score(y_test_original, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_original, y_pred_test))\n",
    "    mae_test = mean_absolute_error(y_test_original, y_pred_test)\n",
    "\n",
    "    # Append results for this train-test-parameter set combination\n",
    "    results.append({\n",
    "        'Train_Dataset': train_name,\n",
    "        'Test_Dataset': test_name,\n",
    "        'Parameter_Set': param_label,\n",
    "        'Model': 'LSTM',\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save model\n",
    "    model_filename = f'lstm_model_{train_name}_trained_on_{test_name}_params_{param_label}.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(lstm_model, model_file)\n",
    "\n",
    "# Loop over each combination of train and test datasets, and parameter sets\n",
    "for train_name, (X_train, y_train, feature_scaler_train, target_scaler_train) in datasets_dict.items():\n",
    "    if \"_Test\" not in train_name:  # Skip test sets in the training loop\n",
    "        for test_name, (X_test, y_test, feature_scaler_test, target_scaler_test) in datasets_dict.items():\n",
    "            if \"_Test\" in test_name:  # Use only test sets in the testing loop\n",
    "                # Evaluate LSTM model with both RGS and Optuna parameter sets\n",
    "                for param_label, params in parameter_sets.items():\n",
    "                    evaluate_and_save_results_lstm(\n",
    "                        train_name, test_name, X_train, X_test, y_train, y_test,\n",
    "                        target_scaler_train, target_scaler_test, param_label, params\n",
    "                    )\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('LSTM_Train_Test_Results_98_combinations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results list to store evaluation metrics\n",
    "results = []\n",
    "\n",
    "# RNN model configurations based on different hyperparameter tuning methods\n",
    "rnn_configs = {\n",
    "    'RNN_Custom': {\n",
    "        'optimizer': 'adam',\n",
    "        'learning_rate': 0.005,\n",
    "        'layers': 2,\n",
    "        'hidden_units': 50,\n",
    "        'grad_clip': 5,\n",
    "        'dropout_rate': 0.1,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    'RNN_Optuna': {\n",
    "        'layers': 2,\n",
    "        'hidden_units': 69,\n",
    "        'dropout_rate': 0.06695188043530183,\n",
    "        'learning_rate': 0.010354065744661594,\n",
    "        'optimizer': 'adam',\n",
    "        'grad_clip': 3,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 24\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to build the RNN model\n",
    "def build_rnn_model(input_shape, params):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for i in range(params['layers']):\n",
    "        return_sequences = i < params['layers'] - 1\n",
    "        model.add(SimpleRNN(params['hidden_units'], return_sequences=return_sequences, \n",
    "                            activation='tanh', \n",
    "                            kernel_constraint=tf.keras.constraints.max_norm(params['grad_clip'])))\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Prepare sequences function with only past 5-day features (exclude today's features)\n",
    "def prepare_sequences(data, sequence_length=5):\n",
    "    X, y = [], []\n",
    "    grouped_data = data.groupby('Symbol')\n",
    "\n",
    "    # Process each company's data separately to prevent mixing companies in sequences\n",
    "    for symbol, group in grouped_data:\n",
    "        group = group.sort_values(by='Date')  # Ensure time order\n",
    "        feature_columns = [col for col in group.columns if col not in ['Date', 'Symbol', 'GICS Sector', 'Headquarters Location', 'Founded']]\n",
    "        features = group[feature_columns].values\n",
    "        target = group['Close'].values\n",
    "\n",
    "        # Create sequences within the current company's data\n",
    "        for i in range(len(features) - sequence_length):\n",
    "            X.append(features[i:i + sequence_length])  # Past 5 days of features\n",
    "            y.append(target[i + sequence_length])      # Target is the current day's close price after the 5-day sequence\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to evaluate model, save results, and append to results\n",
    "def evaluate_and_save_results_rnn(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, rnn_params):\n",
    "    # Initialize and build the RNN model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    rnn_model = build_rnn_model(input_shape, rnn_params)\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = rnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=rnn_params['epochs'],\n",
    "        batch_size=rnn_params['batch_size'],\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict on train and test data and apply inverse transformation\n",
    "    y_pred_train = target_scaler.inverse_transform(rnn_model.predict(X_train).flatten().reshape(-1, 1)).flatten()\n",
    "    y_pred_test = target_scaler.inverse_transform(rnn_model.predict(X_test).flatten().reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Inverse transform y_train and y_test for accurate plotting\n",
    "    y_train_original = target_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train_original, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_original, y_pred_train))\n",
    "    mae_train = mean_absolute_error(y_train_original, y_pred_train)\n",
    "\n",
    "    r2_test = r2_score(y_test_original, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_original, y_pred_test))\n",
    "    mae_test = mean_absolute_error(y_test_original, y_pred_test)\n",
    "\n",
    "    # Append results to the already defined results list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': config_name,\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle\n",
    "    model_filename = f'rnn_model_{config_name}_{method_label}_{dataset_name}.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(rnn_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train_original, y_test_original],\n",
    "        [y_pred_train, y_pred_test],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(y_true)), y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(range(len(y_pred)), y_pred, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'{config_name} - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'actual_vs_predicted_{data_type}_{config_name}_{dataset_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate for each RNN configuration\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        for config_name, rnn_params in rnn_configs.items():\n",
    "            evaluate_and_save_results_rnn(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, rnn_params)\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('RNN_Results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results list to store evaluation metrics\n",
    "results = []\n",
    "\n",
    "# GRU model configurations based on different hyperparameter tuning methods\n",
    "gru_configs = {\n",
    "    'GRU_RandomizedSearch': {\n",
    "        'optimizer': 'adam',\n",
    "        'learning_rate': 0.02,\n",
    "        'layers': 1,\n",
    "        'hidden_units': 100,\n",
    "        'grad_clip': 5,\n",
    "        'dropout_rate': 0.2,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    'GRU_Optuna': {\n",
    "        'layers': 1,\n",
    "        'hidden_units': 116,\n",
    "        'dropout_rate': 0.24446919327565325,\n",
    "        'learning_rate': 0.024719470804780756,\n",
    "        'optimizer': 'adam',\n",
    "        'grad_clip': 5,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to build the GRU model\n",
    "def build_gru_model(input_shape, params):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for i in range(params['layers']):\n",
    "        return_sequences = i < params['layers'] - 1\n",
    "        model.add(GRU(params['hidden_units'], return_sequences=return_sequences, \n",
    "                      activation='tanh', \n",
    "                      kernel_constraint=tf.keras.constraints.max_norm(params['grad_clip'])))\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Prepare sequences function with only past 5-day features (exclude today's features)\n",
    "def prepare_sequences(data, sequence_length=5):\n",
    "    X, y = [], []\n",
    "    grouped_data = data.groupby('Symbol')\n",
    "\n",
    "    # Process each company's data separately to prevent mixing companies in sequences\n",
    "    for symbol, group in grouped_data:\n",
    "        group = group.sort_values(by='Date')  # Ensure time order\n",
    "        feature_columns = [col for col in group.columns if col not in ['Date', 'Symbol', 'GICS Sector', 'Headquarters Location', 'Founded']]\n",
    "        features = group[feature_columns].values\n",
    "        target = group['Close'].values\n",
    "\n",
    "        # Create sequences within the current company's data\n",
    "        for i in range(len(features) - sequence_length):\n",
    "            X.append(features[i:i + sequence_length])  # Past 5 days of features\n",
    "            y.append(target[i + sequence_length])      # Target is the current day's close price after the 5-day sequence\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to evaluate model, save results, and append to results\n",
    "def evaluate_and_save_results_gru(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, gru_params):\n",
    "    # Initialize and build the GRU model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    gru_model = build_gru_model(input_shape, gru_params)\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = gru_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=gru_params['epochs'],\n",
    "        batch_size=gru_params['batch_size'],\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict on train and test data and apply inverse transformation\n",
    "    y_pred_train = target_scaler.inverse_transform(gru_model.predict(X_train).flatten().reshape(-1, 1)).flatten()\n",
    "    y_pred_test = target_scaler.inverse_transform(gru_model.predict(X_test).flatten().reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Inverse transform y_train and y_test for accurate plotting\n",
    "    y_train_original = target_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train = r2_score(y_train_original, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_original, y_pred_train))\n",
    "    mae_train = mean_absolute_error(y_train_original, y_pred_train)\n",
    "\n",
    "    r2_test = r2_score(y_test_original, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_original, y_pred_test))\n",
    "    mae_test = mean_absolute_error(y_test_original, y_pred_test)\n",
    "\n",
    "    # Append results to the already defined results list\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Method': method_label,\n",
    "        'Model': config_name,\n",
    "        'R2_Train': r2_train,\n",
    "        'RMSE_Train': rmse_train,\n",
    "        'MAE_Train': mae_train,\n",
    "        'R2_Test': r2_test,\n",
    "        'RMSE_Test': rmse_test,\n",
    "        'MAE_Test': mae_test\n",
    "    })\n",
    "\n",
    "    # Save the model using pickle\n",
    "    model_filename = f'gru_model_{config_name}_{method_label}_{dataset_name}.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(gru_model, model_file)\n",
    "\n",
    "    # Plot actual vs predicted for both training and testing\n",
    "    for data_type, y_true, y_pred, title_suffix in zip(\n",
    "        ['train', 'test'],\n",
    "        [y_train_original, y_test_original],\n",
    "        [y_pred_train, y_pred_test],\n",
    "        ['Training', 'Testing']\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(y_true)), y_true, label=f'Actual {title_suffix}', color='blue')\n",
    "        plt.plot(range(len(y_pred)), y_pred, label=f'Predicted {title_suffix}', color='red')\n",
    "        plt.title(f'{config_name} - Actual vs Predicted Closing Price ({title_suffix}) - {dataset_name}')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Closing Price')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'actual_vs_predicted_{data_type}_{config_name}_{dataset_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Loop through each dataset type and evaluate for each GRU configuration\n",
    "for method_label, (datasets, y_train, y_test) in datasets_dict.items():\n",
    "    for dataset_name, (X_train, X_test) in datasets.items():\n",
    "        for config_name, gru_params in gru_configs.items():\n",
    "            evaluate_and_save_results_gru(dataset_name, X_train, X_test, y_train, y_test, method_label, config_name, gru_params)\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('GRU_Results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B8CCA68B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2s/stepWARNING:tensorflow:6 out of the last 20 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B8CCA68B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 810ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 733ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 802ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 775ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 736ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 13s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 801ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 999ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 719ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 719ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 984ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 684ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 755ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 984ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 685ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 677ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 921ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 707ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 958ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 704ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 948ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 671ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 993ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 689ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 984ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 740ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 973ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 708ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 707ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 969ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 715ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 955ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 651ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 991ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 740ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1000ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 660ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 952ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 680ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 952ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 643ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 675ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 926ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 674ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 985ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 681ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 922ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 691ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 882ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 660ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 934ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 665ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 953ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 694ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 952ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 700ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 954ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 769ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 705ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 888ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "All plots have been generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load the results DataFrame\n",
    "results_df = pd.read_csv('LSTM_Train_Test_Results_98_combinations.csv')\n",
    "\n",
    "# Define a function to create and save plots for actual vs. predicted values\n",
    "def plot_actual_vs_predicted(y_actual, y_pred, data_type, param_label, train_name, test_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_actual, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'{param_label} - {data_type} Set - {train_name} trained on {test_name}')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    filename = f'actual_vs_predicted_{data_type}_{param_label}_{train_name}_trained_on_{test_name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Loop through each row in results to plot both training and testing data predictions\n",
    "for idx, row in results_df.iterrows():\n",
    "    train_name = row['Train_Dataset']\n",
    "    test_name = row['Test_Dataset']\n",
    "    param_label = row['Parameter_Set']\n",
    "\n",
    "    # Load the corresponding model file\n",
    "    model_filename = f'lstm_model_{train_name}_trained_on_{test_name}_params_{param_label}.pkl'\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        lstm_model = pickle.load(file)\n",
    "\n",
    "    # Use the trained scalers and sequences to plot predictions\n",
    "    for data_type, X, y, target_scaler in [('train', X_train, y_train, target_scaler_train), \n",
    "                                           ('test', X_test, y_test, target_scaler_test)]:\n",
    "        # Make predictions and inverse transform for actual values\n",
    "        y_pred = target_scaler.inverse_transform(lstm_model.predict(X).flatten().reshape(-1, 1)).flatten()\n",
    "        y_actual = target_scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Generate and save the plot\n",
    "        plot_actual_vs_predicted(y_actual, y_pred, data_type, param_label, train_name, test_name)\n",
    "\n",
    "print(\"All plots have been generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots for GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file not found: gru_model_All_Company_trained_on_AAPL_Test_params_Optuna.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_AAPL_Test_params_RGS.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_FOX_Test_params_Optuna.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_FOX_Test_params_RGS.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_IBM_Test_params_Optuna.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_IBM_Test_params_RGS.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_NOW_Test_params_Optuna.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_NOW_Test_params_RGS.pkl. Skipping this combination.\n",
      "Model file not found: gru_model_All_Company_trained_on_REG_Test_params_Optuna.pkl. Skipping this combination.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 213ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 295ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Plot generation complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load the GRU results DataFrame\n",
    "gru_results_df = pd.read_csv('GRU_Train_Test_Results_98_combinations.csv')\n",
    "\n",
    "# Define a function to create and save plots for actual vs. predicted values for GRU\n",
    "def plot_actual_vs_predicted(y_actual, y_pred, data_type, param_label, train_name, test_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_actual, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'{param_label} - {data_type} Set - {train_name} trained on {test_name}')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    filename = f'GRU_actual_vs_predicted_{data_type}_{param_label}_{train_name}_trained_on_{test_name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Loop through each row in results to plot both training and testing data predictions\n",
    "for idx, row in gru_results_df.iterrows():\n",
    "    train_name = row['Train_Dataset']\n",
    "    test_name = row['Test_Dataset']\n",
    "    param_label = row['Parameter_Set']\n",
    "\n",
    "    # Construct the model filename\n",
    "    model_filename = f'gru_model_{train_name}_trained_on_{test_name}_params_{param_label}.pkl'\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if not os.path.exists(model_filename):\n",
    "        print(f\"Model file not found: {model_filename}. Skipping this combination.\")\n",
    "        continue  # Skip to the next row if the file is missing\n",
    "\n",
    "    # Load the GRU model\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        gru_model = pickle.load(file)\n",
    "\n",
    "    # Assume `X_train`, `y_train`, `X_test`, `y_test`, `target_scaler_train`, and `target_scaler_test` are defined\n",
    "    for data_type, X, y, target_scaler in [('train', X_train, y_train, target_scaler_train), \n",
    "                                           ('test', X_test, y_test, target_scaler_test)]:\n",
    "        # Make predictions and inverse transform for actual values\n",
    "        y_pred = target_scaler.inverse_transform(gru_model.predict(X).flatten().reshape(-1, 1)).flatten()\n",
    "        y_actual = target_scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Generate and save the plot\n",
    "        plot_actual_vs_predicted(y_actual, y_pred, data_type, param_label, train_name, test_name)\n",
    "\n",
    "print(\"Plot generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots for RNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 418ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 414ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 363ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 360ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 359ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 359ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 365ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 306ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 350ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 351ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "All RNN plots have been generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Load the RNN results DataFrame\n",
    "rnn_results_df = pd.read_csv('RNN_Train_Test_Results_98_combinations.csv')\n",
    "\n",
    "# Define a function to create and save plots for actual vs. predicted values for RNN\n",
    "def plot_actual_vs_predicted(y_actual, y_pred, data_type, param_label, train_name, test_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_actual, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'{param_label} - {data_type} Set - {train_name} trained on {test_name}')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    filename = f'RNN_actual_vs_predicted_{data_type}_{param_label}_{train_name}_trained_on_{test_name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Loop through each row in results to plot both training and testing data predictions\n",
    "for idx, row in rnn_results_df.iterrows():\n",
    "    train_name = row['Train_Dataset']\n",
    "    test_name = row['Test_Dataset']\n",
    "    param_label = row['Parameter_Set']\n",
    "\n",
    "    # Load the corresponding RNN model file\n",
    "    model_filename = f'rnn_model_{train_name}_trained_on_{test_name}_params_{param_label}.pkl'\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        rnn_model = pickle.load(file)\n",
    "\n",
    "    # Use the trained scalers and sequences to plot predictions\n",
    "    for data_type, X, y, target_scaler in [('train', X_train, y_train, target_scaler_train), \n",
    "                                           ('test', X_test, y_test, target_scaler_test)]:\n",
    "        # Make predictions and inverse transform for actual values\n",
    "        y_pred = target_scaler.inverse_transform(rnn_model.predict(X).flatten().reshape(-1, 1)).flatten()\n",
    "        y_actual = target_scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Generate and save the plot\n",
    "        plot_actual_vs_predicted(y_actual, y_pred, data_type, param_label, train_name, test_name)\n",
    "\n",
    "print(\"All RNN plots have been generated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
